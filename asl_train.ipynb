{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np   \n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split, GroupKFold, StratifiedGroupKFold\n",
    "\n",
    "from typing import Tuple\n",
    "import torch\n",
    "from torch import nn\n",
    "import json\n",
    "from torch.utils.data import Dataset, DataLoader, SequentialSampler, RandomSampler\n",
    "\n",
    "from transformers import get_cosine_schedule_with_warmup, get_linear_schedule_with_warmup\n",
    "\n",
    "# import asl_utils\n",
    "import torch.nn.functional as F\n",
    "from timeit import default_timer as timer\n",
    "from scipy.special import softmax\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import os, gc, pickle, math, time, random, copy, json\n",
    "\n",
    "data_dir = '/home/rashmi/Documents/kaggle/asl_signs/'\n",
    "root_dir = '/home/rashmi/Documents/kaggle/asl_signs/input/'\n",
    "feature_dir = '/home/rashmi/Documents/kaggle/asl_signs/src/exp25/'\n",
    "ext_dir = '/home/rashmi/Documents/kaggle/asl_signs/input/ext_data/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Directory settings\n",
    "# ====================================================\n",
    "import os\n",
    "EXP_NAME = 'exp81'\n",
    "OUTPUT_DIR = f'{data_dir}/src/models_' + EXP_NAME + \"/\"\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)\n",
    "\n",
    "sign_to_label = json.load(open(f\"{root_dir}sign_to_prediction_index_map.json\", \"r\"))\n",
    "\n",
    "np.seterr(invalid='ignore')\n",
    "\n",
    "\n",
    "LHAND = np.arange(468, 489).tolist()\n",
    "RHAND = np.arange(522, 543).tolist()\n",
    "REYE = [\n",
    "    33, 7, 163, 144, 145, 153, 154, 155, 133,\n",
    "    246, 161, 160, 159, 158, 157, 173,\n",
    "]\n",
    "LEYE = [\n",
    "    263, 249, 390, 373, 374, 380, 381, 382, 362,\n",
    "    466, 388, 387, 386, 385, 384, 398,\n",
    "]\n",
    "\n",
    "\n",
    "LIP = [\n",
    "    61, 185, 40, 39, 37, 0, 267, 269, 270, 409,\n",
    "    291, 146, 91, 181, 84, 17, 314, 405, 321, 375,\n",
    "    78, 191, 80, 81, 82, 13, 312, 311, 310, 415,\n",
    "    95, 88, 178, 87, 14, 317, 402, 318, 324, 308,\n",
    "]\n",
    "SPOSE = [504, 502, 500, 501, 503, 505, 512, 513]\n",
    "NOSE=[\n",
    "    1,2,98,327\n",
    "]\n",
    "\n",
    "max_length = 256 #512 #256  ## Number of frames to be considered for each video\n",
    "num_point  = 1050 #960 #82\n",
    "\n",
    "embed_dim  = 384\n",
    "num_head   = 8\n",
    "num_block  = 1\n",
    "\n",
    "start_lr   = 1e-3\n",
    "\n",
    "skip_save_epoch = 0\n",
    "# num_epoch = 50 #200\n",
    "\n",
    "\n",
    "DEBUG = False\n",
    "\n",
    "class Config:\n",
    "    seed = 42\n",
    "    n_fold = 21\n",
    "    trn_fold = [0,4,8,9,10,18] #[0,1,2,3,4]\n",
    "    train = True\n",
    "    TRAIN_CSV = f'{data_dir}input/folds_data/asl_train_21folds_sgkf.csv'\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    epochs = 50\n",
    "    apex = False #True\n",
    "    val_strategy = 'batch'\n",
    "    val_steps = 1000\n",
    "    scheduler = 'cosine'\n",
    "    batch_scheduler=True\n",
    "    batch_size = 1024 #768 #64 \n",
    "    num_warmup_steps = 0.02\n",
    "    num_cycles=0.5\n",
    "    dropout1 = 0.0\n",
    "\n",
    "CFG = Config()\n",
    "df_folds  = pd.read_csv(CFG.TRAIN_CSV) #, nrows=1000)\n",
    "df_folds.loc[:, 'label'] = df_folds.sign.map(sign_to_label)\n",
    "\n",
    "true_cols = ['label']\n",
    "pred_cols = ['pred_label']\n",
    "\n",
    "df_folds['sequence_id'] = df_folds.index\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ROWS_PER_FRAME = 543\n",
    "\n",
    "def load_relevant_data_subset(pq_path):\n",
    "    data_columns = ['x', 'y', 'z']\n",
    "    data = pd.read_parquet(pq_path, columns=data_columns)\n",
    "    n_frames = int(len(data) / ROWS_PER_FRAME)\n",
    "    data = data.values.reshape(n_frames, ROWS_PER_FRAME, len(data_columns))\n",
    "    return data.astype(np.float32)\n",
    "\n",
    "#assum zero-mean one-std, input\n",
    "def do_random_affine(xyz,\n",
    "    scale  = (0.8,1.5),\n",
    "    shift  = (-0.1,0.1),\n",
    "    degree = (-15,15),\n",
    "    p=0.5\n",
    "):\n",
    "\n",
    "    if np.random.rand()<p:\n",
    "        if scale is not None:\n",
    "            scale = np.random.uniform(*scale)\n",
    "            xyz = scale*xyz\n",
    "\n",
    "        if shift is not None:\n",
    "            shift = np.random.uniform(*shift)\n",
    "            xyz = xyz + shift\n",
    "\n",
    "        if degree is not None:\n",
    "            degree = np.random.uniform(*degree)\n",
    "            radian = degree/180*np.pi\n",
    "            c = np.cos(radian)\n",
    "            s = np.sin(radian)\n",
    "            rotate = np.array([\n",
    "                [c,-s],\n",
    "                [s, c],\n",
    "            ]).T\n",
    "            xyz[...,:2] = xyz[...,:2] @rotate\n",
    "\n",
    "    return xyz\n",
    "\n",
    "#-----------------------------------------------------\n",
    "def train_augment(xyz):\n",
    "    xyz = do_random_affine(\n",
    "        xyz,\n",
    "        scale  = (0.7,1.3),\n",
    "        shift  = (-0.08,0.08),\n",
    "        degree = (-20,20),\n",
    "        p=0.5\n",
    "    )\n",
    "    return xyz\n",
    "\n",
    "\n",
    "def pre_process(xyz):\n",
    "    #xyz = xyz - xyz[~torch.isnan(xyz)].mean(0,keepdims=True) #noramlisation to common maen\n",
    "    #xyz = xyz / xyz[~torch.isnan(xyz)].std(0, keepdims=True)\n",
    "    lip   = xyz[:, LIP]\n",
    "    lhand = xyz[:, LHAND]\n",
    "    rhand = xyz[:, RHAND]\n",
    "\n",
    "    xyz = torch.cat([ #(none, 82, 3)\n",
    "        lip,\n",
    "        lhand,\n",
    "        rhand,\n",
    "    ],1)\n",
    "    xyz[torch.isnan(xyz)] = 0\n",
    "    xyz = xyz[:max_length]  ## Limiting number of frames to max_length\n",
    "    return xyz\n",
    "\n",
    "\n",
    "    ...\n",
    "## assume zero mean xyz. so flip can be implement by multuiplication of -1\n",
    "def do_hflip_hand(lhand, rhand):\n",
    "    rhand[...,0] *= -1\n",
    "    lhand[...,0] *= -1\n",
    "    rhand, lhand = lhand,rhand\n",
    "    return lhand, rhand\n",
    "\n",
    "def do_hflip_spose(spose):\n",
    "    spose[...,0] *= -1\n",
    "    spose = spose[:,[3,4,5,0,1,2,7,6]]\n",
    "    return spose\n",
    "\n",
    "def do_hflip_slip(slip):\n",
    "    slip[...,0] *= -1\n",
    "    slip = slip[:,[10,9,8,7,6,5,4,3,2,1,0]+[19,18,17,16,15,14,13,12,11]]\n",
    "    return slip\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "offset = (np.arange(1000)- max_length)//2\n",
    "offset = np.clip(offset,0, 1000).tolist()\n",
    "\n",
    "triu_index= [\n",
    "\t\t\t1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,\n",
    "\t\t\t14, 15, 16, 17, 18, 19, 20, 23, 24, 25, 26, 27, 28,\n",
    "\t\t\t29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41,\n",
    "\t\t\t45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57,\n",
    "\t\t\t58, 59, 60, 61, 62, 67, 68, 69, 70, 71, 72, 73, 74,\n",
    "\t\t\t75, 76, 77, 78, 79, 80, 81, 82, 83, 89, 90, 91, 92,\n",
    "\t\t\t93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 111,\n",
    "\t\t\t112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124,\n",
    "\t\t\t125, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144,\n",
    "\t\t\t145, 146, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165,\n",
    "\t\t\t166, 167, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187,\n",
    "\t\t\t188, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 221,\n",
    "\t\t\t222, 223, 224, 225, 226, 227, 228, 229, 230, 243, 244, 245, 246,\n",
    "\t\t\t247, 248, 249, 250, 251, 265, 266, 267, 268, 269, 270, 271, 272,\n",
    "\t\t\t287, 288, 289, 290, 291, 292, 293, 309, 310, 311, 312, 313, 314,\n",
    "\t\t\t331, 332, 333, 334, 335, 353, 354, 355, 356, 375, 376, 377, 397,\n",
    "\t\t\t398, 419,\n",
    "\t\t]\n",
    "\n",
    "\n",
    "def do_normalise_by_ref(xyz, ref):  \n",
    "    K = xyz.shape[-1]\n",
    "    xyz_flat = ref.reshape(-1,K)\n",
    "\n",
    "    m = np.nanmean(xyz_flat,0).reshape(1,1,K)\n",
    "    s = np.nanstd(xyz_flat, 0).reshape(1,1,K) # .mean() \n",
    "    xyz = xyz - m\n",
    "    xyz = xyz / s\n",
    "    return xyz\n",
    "\n",
    "\n",
    "## assume zero mean xyz. so flip can be implement by multuiplication of -1\n",
    "def do_hflip_hand(lhand, rhand):\n",
    "    rhand[...,0] *= -1\n",
    "    lhand[...,0] *= -1\n",
    "    rhand, lhand = lhand,rhand\n",
    "    return lhand, rhand\n",
    "\n",
    "def do_hflip_spose(spose):\n",
    "    spose[...,0] *= -1\n",
    "    spose = spose[:,[3,4,5,0,1,2,7,6]]\n",
    "    return spose\n",
    "\n",
    "def do_hflip_slip(slip):\n",
    "    slip[...,0] *= -1\n",
    "    slip = slip[:,[10,9,8,7,6,5,4,3,2,1,0]+[19,18,17,16,15,14,13,12,11]]\n",
    "    return slip\n",
    "\n",
    "FLIP_LABELS = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, \\\n",
    "\t27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, \\\n",
    "\t55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 69, 70, 71, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, \\\n",
    "\t84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, \\\n",
    "\t110, 113, 114, 115, 116, 117, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, \\\n",
    "\t136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 155, 156, 157, 158, \\\n",
    "\t159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, \\\n",
    "\t181, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 198, 199, 200, 201, 203, 205, 206, \\\n",
    "\t207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, \\\n",
    "\t230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 241, 242, 243, 244, 245, 246, 247, 248, 249]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SignDataset(Dataset):\n",
    "    def __init__(self, df, augment=None):\n",
    "        self.df = df\n",
    "        self.augment = augment\n",
    "        self.length = len(self.df)\n",
    "\n",
    "    def __str__(self):\n",
    "        # num_participant_id = self.df.participant_id.nunique()\n",
    "        # string = ''\n",
    "        # string += f'\\tlen = {len(self)}\\n'\n",
    "        # string += f'\\tnum_participant_id = {num_participant_id}\\n'\n",
    "        # return string\n",
    "        return \"Not working yet\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        d = self.df.iloc[index]\n",
    "        \n",
    "        root_dir_new=root_dir\n",
    "        pq_file = f'{root_dir_new}{d.path}'\n",
    "\n",
    "        xyz = load_relevant_data_subset(pq_file)\n",
    "        xyz = torch.from_numpy(xyz).float()\n",
    "\n",
    "       \n",
    "        L = len(xyz)\n",
    "        if L>max_length:\n",
    "            #xyz = xyz[:self.max_length] #first\n",
    "            #xyz = xyz[-self.max_length:] #last\n",
    "            i = offset[L]\n",
    "            xyz = xyz[i:i+max_length] #center\n",
    "        \n",
    "        L = len(xyz)\n",
    "        REF = LIP + SPOSE + LHAND + RHAND \n",
    "        xyz = do_normalise_by_ref(xyz, xyz[:,REF])\n",
    "\n",
    "      \n",
    "        lhand = xyz[:,LHAND]\n",
    "        rhand = xyz[:,RHAND]\n",
    "      \n",
    "        if self.augment is not None:\n",
    "            # if d.label in FLIP_LABELS:\n",
    "            if np.random.rand()<0.5:\n",
    "                lhand, rhand = do_hflip_hand(lhand, rhand)\n",
    "\n",
    "                xyz[:,LHAND] = lhand\n",
    "                xyz[:,RHAND] = rhand\n",
    "        \n",
    "\n",
    "\n",
    "        # add distance\n",
    "        lhand2 = lhand[:, :21, :2]\n",
    "        ld = lhand2.reshape(-1, 21, 1, 2) - lhand2.reshape(-1, 1, 21, 2)\n",
    "        ld = np.sqrt((ld ** 2).sum(-1))\n",
    "        ld = ld.reshape(L, -1)\n",
    "        ld = ld[:,triu_index]\n",
    "        \n",
    "        rhand2 = rhand[:, :21, :2]\n",
    "        rd = rhand2.reshape(-1, 21, 1, 2) - rhand2.reshape(-1, 1, 21, 2)\n",
    "        rd = np.sqrt((rd ** 2).sum(-1))\n",
    "        rd = rd.reshape(L, -1)\n",
    "        rd = rd[:,triu_index]\n",
    "\n",
    "        xyz = torch.cat([  # (none, 82, 3)\n",
    "            lhand,\n",
    "            rhand,\n",
    "            xyz[:, LIP],\n",
    "            xyz[:, SPOSE],\n",
    "            \n",
    "        ], 1).contiguous()\n",
    "        \n",
    "        dxyz = F.pad(xyz[:-1] - xyz[1:], [0, 0, 0, 0, 0, 1])\n",
    "\n",
    "        a = xyz[1:,:,0] - xyz[:-1,:,0]\n",
    "        b = xyz[1:,:,1] - xyz[:-1,:,1]\n",
    "        axyz = ((a**2 + b**2)**0.5)\n",
    "        axyz = F.pad(axyz, [0, 0, 0, 1])\n",
    "\n",
    "        x = torch.cat([\n",
    "            xyz.reshape(L,-1),\n",
    "            dxyz.reshape(L,-1),\n",
    "            axyz.reshape(L,-1),  \n",
    "            rd.reshape(L,-1),\n",
    "            ld.reshape(L,-1),\n",
    "        ], -1)\n",
    "        x[torch.isnan(x)] = 0\n",
    "                    \n",
    "       \n",
    "        r = {}\n",
    "        r['index'] = d.index\n",
    "        r['d'    ] = d\n",
    "        r['xyz'  ] = x\n",
    "        r['label'] = d.label\n",
    "        return r\n",
    "    \n",
    "    \n",
    "tensor_key = ['xyz', 'label', 'index']\n",
    "def null_collate(batch):\n",
    "    batch_size = len(batch)\n",
    "    d = {}\n",
    "    key = batch[0].keys()\n",
    "    for k in key:\n",
    "        d[k] = [b[k] for b in batch]\n",
    "    d['label'] = torch.LongTensor(d['label'])\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not working yet\n",
      "batch_size   : 8\n",
      "len(loader)  : 11227\n",
      "len(dataset) : 89821\n",
      "\n",
      "batch  0 ===================\n",
      "index [Index(['path', 'participant_id', 'sequence_id', 'sign', 'fold', 'label'], dtype='object'), Index(['path', 'participant_id', 'sequence_id', 'sign', 'fold', 'label'], dtype='object'), Index(['path', 'participant_id', 'sequence_id', 'sign', 'fold', 'label'], dtype='object'), Index(['path', 'participant_id', 'sequence_id', 'sign', 'fold', 'label'], dtype='object'), Index(['path', 'participant_id', 'sequence_id', 'sign', 'fold', 'label'], dtype='object'), Index(['path', 'participant_id', 'sequence_id', 'sign', 'fold', 'label'], dtype='object'), Index(['path', 'participant_id', 'sequence_id', 'sign', 'fold', 'label'], dtype='object'), Index(['path', 'participant_id', 'sequence_id', 'sign', 'fold', 'label'], dtype='object')]\n",
      "xyz:\n",
      "\t torch.Size([23, 1050])\n",
      "\t torch.Size([11, 1050])\n",
      "\t torch.Size([105, 1050])\n",
      "\t torch.Size([12, 1050])\n",
      "\t torch.Size([18, 1050])\n",
      "\t torch.Size([30, 1050])\n",
      "\t torch.Size([23, 1050])\n",
      "\t torch.Size([57, 1050])\n",
      "label:\n",
      "\t [25, 232, 48, 23, 164, 67, 143, 134]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def run_check_dataset():\n",
    "\n",
    "    fold = 0 \n",
    "    train_df = df_folds[df_folds.fold!=fold].reset_index(drop=True)\n",
    "    valid_df = df_folds[df_folds.fold==fold].reset_index(drop=True) \n",
    "\n",
    "    dataset = SignDataset(train_df, augment=True)\n",
    "    print(dataset)\n",
    "\n",
    "    # for i in range(12):\n",
    "    #     r = dataset[i]\n",
    "    #     print(r['index'], '--------------------')\n",
    "    #     print(r[\"d\"], '\\n')\n",
    "    #     for k in tensor_key:\n",
    "    #         if k =='label': continue\n",
    "    #         v = r[k]\n",
    "    #         print(k)\n",
    "    #         print('\\t', 'dtype:', v.dtype)\n",
    "    #         print('\\t', 'shape:', v.shape)\n",
    "    #         if len(v)!=0:\n",
    "    #             print('\\t', 'min/max:', v.min().item(),'/', v.max().item())\n",
    "    #             print('\\t', 'is_contiguous:', v.is_contiguous())\n",
    "    #             print('\\t', 'values:')\n",
    "    #             print('\\t\\t', v.reshape(-1)[:5].data.numpy().tolist(), '...')\n",
    "    #             print('\\t\\t', v.reshape(-1)[-5:].data.numpy().tolist())\n",
    "    #     print('')\n",
    "       \n",
    "\n",
    "    loader = DataLoader(\n",
    "        dataset,\n",
    "        sampler=SequentialSampler(dataset),\n",
    "        batch_size=8,\n",
    "        drop_last=True,\n",
    "        num_workers=0,\n",
    "        pin_memory=False,\n",
    "        worker_init_fn=lambda id: np.random.seed(torch.initial_seed() // 2 ** 32 + id),\n",
    "        collate_fn=null_collate,\n",
    "    )\n",
    "    print(f'batch_size   : {loader.batch_size}')\n",
    "    print(f'len(loader)  : {len(loader)}')\n",
    "    print(f'len(dataset) : {len(dataset)}')\n",
    "    print('')\n",
    "\n",
    "    for t, batch in enumerate(loader):\n",
    "        if t > 0: break\n",
    "        print('batch ', t, '===================')\n",
    "        print('index', batch['index'])\n",
    "\n",
    "        for k in tensor_key:\n",
    "            v = batch[k]\n",
    "\n",
    "            if k =='label':\n",
    "                print('label:')\n",
    "                print('\\t', v.data.numpy().tolist())\n",
    "\n",
    "            if k =='x':\n",
    "                print('x:')\n",
    "                print('\\t', v.data.shape)\n",
    "\n",
    "            if k =='xyz':\n",
    "                print('xyz:')\n",
    "                for i in range(len(v)):\n",
    "                    print('\\t', v[i].shape)\n",
    "\n",
    "        if 1:\n",
    "            pass\n",
    "        print('')\n",
    "\n",
    "\n",
    "# main #################################################################\n",
    "if 1: #DEBUG:\n",
    "    run_check_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch\n",
      "                           label : torch.Size([4]) \n",
      "                             xyz : torch.Size([12, 1050]) \n",
      "                                 : torch.Size([16, 1050]) \n",
      "                                 : torch.Size([20, 1050]) \n",
      "                                 : torch.Size([180, 1050]) \n",
      "output\n",
      "                           logit : torch.Size([4, 250]) \n",
      "                            sign : torch.Size([4, 250]) \n",
      "loss\n"
     ]
    }
   ],
   "source": [
    "num_class  = 250\n",
    "point_dim = num_point\n",
    "\n",
    "def pack_seq(\n",
    "    seq,):\n",
    "    length = [min(len(s), max_length)  for s in seq]\n",
    "    batch_size = len(seq)\n",
    "    K = seq[0].shape[1]\n",
    "    L = max(length)\n",
    "\n",
    "    x = torch.zeros((batch_size, L, point_dim)).to(seq[0].device)\n",
    "    x_mask = torch.zeros((batch_size, L)).to(seq[0].device)\n",
    "    for b in range(batch_size):\n",
    "        l = length[b]\n",
    "        x[b, :l] = seq[b][:l,:]\n",
    "        x_mask[b, l:] = 1\n",
    "    x_mask = (x_mask>0.5)\n",
    "\n",
    "    return x, x_mask\n",
    "\n",
    "def positional_encoding(length, embed_dim):\n",
    "    dim = embed_dim//2\n",
    "    position = np.arange(length)[:, np.newaxis]     # (seq, 1)\n",
    "    dim = np.arange(dim)[np.newaxis, :]/dim   # (1, dim)\n",
    "    angle = 1 / (10000**dim)         # (1, dim)\n",
    "    angle = position * angle    # (pos, dim)\n",
    "    pos_embed = np.concatenate(\n",
    "        [np.sin(angle), np.cos(angle)],\n",
    "        axis=-1\n",
    "    )\n",
    "    pos_embed = torch.from_numpy(pos_embed).float()\n",
    "    return pos_embed\n",
    "\n",
    "\n",
    "class XEmbed(nn.Module):\n",
    "    def __init__(self,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.v = nn.Sequential(\n",
    "            nn.Linear(point_dim, embed_dim*2, bias=True),\n",
    "            nn.LayerNorm(embed_dim*2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(embed_dim*2, embed_dim, bias=True),\n",
    "            nn.LayerNorm(embed_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "    def forward(self, x, x_mask):\n",
    "        B,L,_ = x.shape\n",
    "        v = self.v(x)\n",
    "        x = v\n",
    "        return x, x_mask\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self,\n",
    "        embed_dim,\n",
    "        num_head,\n",
    "        out_dim,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.attn  = MyMultiHeadAttention(\n",
    "            embed_dim=embed_dim,\n",
    "            out_dim=embed_dim,\n",
    "            qk_dim=embed_dim // num_head,\n",
    "            v_dim=embed_dim // num_head,\n",
    "            num_head=num_head,\n",
    "\n",
    "        )\n",
    "        self.ffn   = FeedForward(embed_dim, out_dim)\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(out_dim)\n",
    "\n",
    "    def forward(self, x, x_mask=None):\n",
    "        x = x + self.attn((self.norm1(x)), x_mask)\n",
    "        x = x + self.ffn((self.norm2(x)))\n",
    "        return x\n",
    "    \n",
    "class MyMultiHeadAttention(nn.Module):\n",
    "    def __init__(self,\n",
    "            embed_dim,\n",
    "            out_dim,\n",
    "            qk_dim,\n",
    "            v_dim,\n",
    "            num_head,\n",
    "        ):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_head  = num_head\n",
    "        self.qk_dim = qk_dim\n",
    "        self.v_dim  = v_dim\n",
    "\n",
    "        self.q = nn.Linear(embed_dim, qk_dim*num_head)\n",
    "        self.k = nn.Linear(embed_dim, qk_dim*num_head)\n",
    "        self.v = nn.Linear(embed_dim, v_dim*num_head)\n",
    "\n",
    "        self.out = nn.Linear(v_dim*num_head, out_dim)\n",
    "        self.scale = 1/(qk_dim**0.5)\n",
    "\n",
    "    #https://github.com/pytorch/pytorch/issues/40497\n",
    "    def forward(self, x, x_mask):\n",
    "        B,L,dim = x.shape\n",
    "        #out, _ = self.mha(x,x,x, key_padding_mask=x_mask)\n",
    "        num_head = self.num_head\n",
    "        qk_dim = self.qk_dim\n",
    "        v_dim = self.v_dim\n",
    "\n",
    "        q = self.q(x)\n",
    "        k = self.k(x)\n",
    "        v = self.v(x)\n",
    "        q = q.reshape(B, L, num_head, qk_dim).permute(0,2,1,3).contiguous()\n",
    "        k = k.reshape(B, L, num_head, qk_dim).permute(0,2,3,1).contiguous()\n",
    "        v = v.reshape(B, L, num_head, v_dim ).permute(0,2,1,3).contiguous()\n",
    "\n",
    "        dot = torch.matmul(q, k) *self.scale  # H L L\n",
    "        x_mask = x_mask.reshape(B,1,1,L).expand(-1,num_head,L,-1)\n",
    "        #dot[x_mask]= -1e4\n",
    "        dot.masked_fill_(x_mask, -1e4)\n",
    "        attn = F.softmax(dot, -1)    # L L\n",
    "\n",
    "        v = torch.matmul(attn, v)  # L H dim\n",
    "        v = v.permute(0,2,1,3).reshape(B,L, v_dim*num_head).contiguous()\n",
    "        out = self.out(v)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, embed_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, hidden_dim),\n",
    "            nn.LayerNorm(embed_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(hidden_dim, embed_dim),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.mlp(x)\n",
    "    \n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self, num_class=num_class):\n",
    "        super().__init__()\n",
    "        self.output_type = ['inference', 'loss']\n",
    "\n",
    "        self.x_embed = XEmbed()\n",
    "\n",
    "        pos_embed = positional_encoding(max_length,16) #max_length, embed_dim)\n",
    "        self.pos_embed = nn.Parameter(pos_embed) \n",
    "\n",
    "        # Linear layer to map pos_enc_dim to emb_dim\n",
    "        self.pos_enc_linear = nn.Linear(16, embed_dim)\n",
    "\n",
    "        self.cls_embed = nn.Parameter(torch.zeros((1, embed_dim)))\n",
    "\n",
    "        self.encoder = nn.ModuleList([\n",
    "            TransformerBlock(\n",
    "                embed_dim,\n",
    "                num_head,\n",
    "                embed_dim,\n",
    "            ) for i in range(num_block)\n",
    "        ])\n",
    "\n",
    "        self.seq_layer = nn.Sequential(\n",
    "            nn.Linear(embed_dim*2, embed_dim),\n",
    "            nn.BatchNorm1d(embed_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.class_layer = nn.Linear(embed_dim, num_class)\n",
    "        \n",
    "\n",
    "    def forward(self, batch):\n",
    "        xyz = batch['xyz']\n",
    "\n",
    "        #----\n",
    "        x, x_mask = pack_seq(xyz)\n",
    "        x, x_mask = self.x_embed(x, x_mask)\n",
    "        B,L,_ = x.shape\n",
    "\n",
    "        x = x + self.pos_enc_linear(self.pos_embed[:L].unsqueeze(0))\n",
    "\n",
    "        x = torch.cat([\n",
    "            self.cls_embed.unsqueeze(0).repeat(B,1,1),\n",
    "            x\n",
    "        ],1)\n",
    "        x_mask = torch.cat([\n",
    "            torch.zeros(B,1).to(x_mask),\n",
    "            x_mask\n",
    "        ],1)\n",
    "        \n",
    "        for block in self.encoder:\n",
    "            x = block(x,x_mask)\n",
    "        x = F.dropout(x,p=CFG.dropout1,training=self.training)\n",
    "\n",
    "        #---\n",
    "        #mask pool\n",
    "        x_mask = x_mask.unsqueeze(-1)\n",
    "        x_mask = 1-x_mask.float()\n",
    "        last = (x*x_mask).sum(1)/x_mask.sum(1)\n",
    "       \n",
    "        \n",
    "        x = torch.cat([x[:,0],last],1)\n",
    "        \n",
    "        x = self.seq_layer(x)\n",
    "        x = F.dropout(x,p=CFG.dropout1, training=self.training)\n",
    "        logit = self.class_layer(x)\n",
    "\n",
    "        output = {}\n",
    "        output['logit'] = logit\n",
    " \n",
    "        if 'inference' in self.output_type:\n",
    "            output['sign'] = torch.softmax(logit,-1)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "def run_check_net():\n",
    "\n",
    "    length = [12,16,20,180]\n",
    "    batch_size = len(length)\n",
    "    xyz = [\n",
    "        np.random.uniform(-1,1,(length[b],num_point)) for b in range(batch_size)\n",
    "    ]\n",
    "    #---\n",
    "    batch = {\n",
    "        'label' : torch.from_numpy( np.random.choice(250,(batch_size))).to(CFG.device).long(),\n",
    "        'xyz' : [torch.from_numpy(x).to(CFG.device).float() for x in xyz]\n",
    "    }\n",
    "\n",
    "    net = Net().to(CFG.device)\n",
    "    output = net(batch)\n",
    "\n",
    "\n",
    "    #---\n",
    "\n",
    "    print('batch')\n",
    "    for k, v in batch.items():\n",
    "        if k in ['label','x']:\n",
    "            print(f'{k:>32} : {v.shape} ')\n",
    "        if k=='xyz':\n",
    "            print(f'{k:>32} : {v[0].shape} ')\n",
    "            for i in range(1,len(v)):\n",
    "                print(f'{\" \":>32} : {v[i].shape} ')\n",
    "\n",
    "    print('output')\n",
    "    for k, v in output.items():\n",
    "        if 'loss' not in k:\n",
    "            print(f'{k:>32} : {v.shape} ')\n",
    "    print('loss')\n",
    "    for k, v in output.items():\n",
    "        if 'loss' in k:\n",
    "            print(f'{k:>32} : {v.item()} ')\n",
    "\n",
    "\n",
    "\n",
    "# main #################################################################\n",
    "if 1: #DEBUG:\n",
    "    run_check_net()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Helper functions\n",
    "# ====================================================\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (remain %s)' % (asMinutes(s), asMinutes(rs))\n",
    "\n",
    "\"\"\"# Random seed\"\"\"\n",
    "\n",
    "def seed_everything(seed, use_cuda = True):\n",
    "    np.random.seed(seed) # cpu vars\n",
    "    torch.manual_seed(seed) # cpu  vars\n",
    "    random.seed(seed) # Python\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed) \n",
    "    if use_cuda:\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True  \n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "def get_logger(filename=OUTPUT_DIR+'train'):\n",
    "    from logging import getLogger, INFO, StreamHandler, FileHandler, Formatter\n",
    "    logger = getLogger(__name__)\n",
    "    logger.setLevel(INFO)\n",
    "    handler1 = StreamHandler()\n",
    "    handler1.setFormatter(Formatter(\"%(message)s\"))\n",
    "    handler2 = FileHandler(filename=f\"{filename}.log\")\n",
    "    handler2.setFormatter(Formatter(\"%(message)s\"))\n",
    "    logger.addHandler(handler1)\n",
    "    logger.addHandler(handler2)\n",
    "    return logger\n",
    "\n",
    "LOGGER = get_logger()\n",
    "\n",
    "seed_everything(CFG.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score(outputs,targets): #outputs=preds, targets=groundtruth\n",
    "    return accuracy_score(targets, outputs)\n",
    "\n",
    "\n",
    "# ====================================================\n",
    "# scheduler\n",
    "# ====================================================\n",
    "def get_scheduler(cfg, optimizer, num_train_steps):\n",
    "    if cfg.scheduler == 'linear':\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer, num_warmup_steps=cfg.num_warmup_steps, num_training_steps=num_train_steps\n",
    "        )\n",
    "    elif cfg.scheduler == 'cosine':\n",
    "        scheduler = get_cosine_schedule_with_warmup(\n",
    "            optimizer, num_warmup_steps=cfg.num_warmup_steps, num_training_steps=num_train_steps, num_cycles=cfg.num_cycles\n",
    "        )\n",
    "    return scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def np_cross_entropy(probability, truth):\n",
    "\tp = np.clip(probability,1e-4,1-1e-4)\n",
    "\tlogp = -np.log(p)\n",
    "\tloss = logp[np.arange(len(logp)),truth]\n",
    "\tloss = loss.mean()\n",
    "\treturn loss\n",
    "\n",
    "def valid_fn(model, valid_loader, valid_idx, iteration):\n",
    "\n",
    "\tvalid_num = 0\n",
    "\tvalid_sign = []\n",
    "\tvalid_loss = 0\n",
    "\n",
    "\tmodel = model.eval()\n",
    "\tstart_timer = timer()\n",
    "\tfor t, batch in enumerate(valid_loader):\n",
    "\t\t\n",
    "\t\tmodel.output_type = ['inference']\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\twith torch.cuda.amp.autocast(enabled = True):\n",
    "\t\t\t\t\n",
    "\t\t\t\tbatch_size = len(batch['index'])\n",
    "\t\t\t\tbatch['xyz'] = [xyz.to(CFG.device) for xyz in batch['xyz']]\n",
    "\t\t\t\toutput = model(batch)  \n",
    "\n",
    "\t\tvalid_sign.append(output['sign'].cpu().numpy())\n",
    "\t\tvalid_num += batch_size\n",
    "\n",
    "\n",
    "\n",
    "\tassert(valid_num == len(valid_loader.dataset))\n",
    "\t#------\n",
    "\ttruth = valid_loader.dataset.df.label.values\n",
    "\tsign = np.concatenate(valid_sign)\n",
    "\tpredict = np.argsort(-sign, -1)\n",
    "\tcorrect = predict==truth.reshape(valid_num,1)\n",
    "\ttopk = correct.cumsum(-1).mean(0)[:5]\n",
    "\n",
    "\tloss = np_cross_entropy(sign, truth)\n",
    "\t\n",
    "\toutput_map = {}\n",
    "\tfor x, y in zip(valid_idx, sign):\n",
    "\t\toutput_map[x] = y\n",
    "\n",
    "\treturn [loss, topk[0], topk[1],  topk[4], sign, output_map]\n",
    "\n",
    "\n",
    "if DEBUG:\n",
    "\tfold  = 4\n",
    "\tvalid_df = df_folds[df_folds.fold==fold].reset_index(drop=True) \n",
    "\n",
    "\tvalid_idx = valid_df.sequence_id.tolist()\n",
    "\tvalid_dataset = SignDataset(valid_df,)\n",
    "\n",
    "\tvalid_loader = DataLoader(\n",
    "\t\tvalid_dataset, shuffle=False,\n",
    "\t\tsampler = SequentialSampler(valid_dataset),\n",
    "\t\tbatch_size  = 64,\n",
    "\t\tdrop_last   = False,\n",
    "\t\tnum_workers = 8,\n",
    "\t\tpin_memory  = False,\n",
    "\t\tcollate_fn = null_collate,\n",
    "\t)\n",
    "\n",
    "\tcfile = '/home/rashmi/Documents/kaggle/asl_signs/src/models_exp4/asl_model_fold1.pth'\n",
    "\tcfile = '/home/rashmi/Documents/kaggle/asl_signs/src/models_exp18/asl_model_fold4.pth'\n",
    "\tf = torch.load(cfile, map_location=lambda storage, loc: storage)\n",
    "\tstate_dict = f['model']\n",
    "\t# state_dict['pos_embed'] = state_dict['pos_embed'][:max_length]\n",
    "\n",
    "\tmodel = Net().to(CFG.device)\n",
    "\tmodel.load_state_dict(state_dict, strict=False) \n",
    "\tmodel.eval()\n",
    "\tvalid_info= valid_fn(model, valid_loader, valid_idx, iteration=100)\n",
    "\tprint(accuracy_score(valid_df.label,np.argmax(valid_info[4],axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(fold, train_loader, model, criterion, optimizer, epoch, scheduler, device, \\\n",
    "             valid_loader=None, valid_idx=None, best_score=0, len_train=94000):\n",
    "    model.train()\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=CFG.apex)\n",
    "    losses = AverageMeter()\n",
    "    start = end = time.time()\n",
    "    global_step = 0\n",
    "\n",
    "    train_loss_sum = 0.\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    \n",
    "    LOGGER.info(f\"========== Epoch: {epoch} training ==========\")\n",
    "\n",
    "    for step, batch in enumerate(train_loader):\n",
    "        model.train()\n",
    "        gc.collect()\n",
    "        \n",
    "        xyz = batch['xyz']\n",
    "        x = [torch.Tensor(x).to(CFG.device).float() for x in xyz]\n",
    "        y = torch.Tensor(batch['label']).long().to(CFG.device)\n",
    "        b = {'label' : y, 'xyz': x}\n",
    "\n",
    "        with torch.cuda.amp.autocast(enabled=CFG.apex):\n",
    "            y_pred = model(b)\n",
    "            loss = criterion(y_pred['logit'], y)\n",
    "        \n",
    "        if CFG.apex:\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        global_step += 1\n",
    "        if CFG.batch_scheduler:\n",
    "            scheduler.step()\n",
    "\n",
    "        train_loss_sum += loss.item()\n",
    "        train_correct += np.sum((np.argmax(y_pred['logit'].detach().cpu().numpy(), axis=1) == y.cpu().numpy()))\n",
    "        train_total += 1\n",
    "\n",
    "    end = time.time()\n",
    "    LOGGER.info(f\"Epoch:{epoch} > Train Loss: {(train_loss_sum/train_total):.04f}, Train Acc: {train_correct/len_train:0.04f}\")\n",
    "    return train_loss_sum, best_score\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# train loop\n",
    "# ====================================================\n",
    "def train_loop(folds, fold):\n",
    "     \n",
    "    seed_everything(seed=CFG.seed)\n",
    "\n",
    "    LOGGER.info(f\"========== fold: {fold} training ==========\")\n",
    "\n",
    "    device = CFG.device\n",
    "    \n",
    "    train_df = folds[folds.fold!=fold].reset_index(drop=True)\n",
    "    valid_df = folds[folds.fold==fold].reset_index(drop=True) \n",
    "\n",
    "    valid_idx = valid_df.sequence_id.tolist()\n",
    "    train_dataset = SignDataset(train_df,train_augment)\n",
    "    valid_dataset = SignDataset(valid_df,)\n",
    "\n",
    "    len_train = len(train_dataset)\n",
    "    train_loader  = DataLoader(\n",
    "        train_dataset,\n",
    "        sampler = RandomSampler(train_dataset),\n",
    "        #sampler = BalanceSampler(train_dataset),\n",
    "        batch_size  = CFG.batch_size,\n",
    "        drop_last   = True,\n",
    "        num_workers = 8,\n",
    "        pin_memory  = True,\n",
    "        worker_init_fn = lambda id: np.random.seed(torch.initial_seed() // 2 ** 32 + id),\n",
    "        collate_fn = null_collate,\n",
    "    )\n",
    "\n",
    "    valid_loader = DataLoader(\n",
    "        valid_dataset, shuffle=False,\n",
    "        sampler = SequentialSampler(valid_dataset),\n",
    "        batch_size  = 64,\n",
    "        drop_last   = False,\n",
    "        num_workers = 8,\n",
    "        pin_memory  = True,\n",
    "        collate_fn = null_collate,\n",
    "    )\n",
    "\n",
    "    model = Net().to(CFG.device)\n",
    "    \n",
    "    optimizer = torch.optim.AdamW( model.parameters(), lr=start_lr, weight_decay=1e-4)\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=0.75)\n",
    "    num_train_steps = int(len(train_df) / CFG.batch_size * CFG.epochs)\n",
    "    scheduler = get_scheduler(CFG, optimizer, num_train_steps)\n",
    "    \n",
    "    num_iteration = CFG.epochs*len(train_loader)\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled = CFG.apex)\t\n",
    "\n",
    "    best_score = 0\n",
    "\n",
    "    for epoch in range(CFG.epochs):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        if epoch < 15:\n",
    "            CFG.dropout1=0.0\n",
    "        else:\n",
    "            if epoch >=15 and epoch <=35:\n",
    "                CFG.dropout1=0.4\n",
    "            else:\n",
    "                CFG.dropout1=0.2\n",
    "        \n",
    "        # train and validate\n",
    "        avg_loss, best_score = train_fn(fold, train_loader, model, criterion, optimizer, epoch, \\\n",
    "                                        scheduler, device, valid_loader, valid_idx, best_score, len_train)\n",
    "\n",
    "        # eval\n",
    "        valid_info =  valid_fn(model, valid_loader, valid_idx, iteration=100) \n",
    "        # scoring\n",
    "        valid_df = df_folds[df_folds.sequence_id.isin(valid_idx)].copy()\n",
    "        output_map = valid_info[5]\n",
    "        valid_df.loc[:, 'pred_label'] = valid_df['sequence_id'].apply(lambda x: np.argmax(output_map[x]))\n",
    "\n",
    "        valid_labels = valid_df[true_cols].values\n",
    "        valid_preds = valid_df[pred_cols].values\n",
    "        score = get_score(np.argmax(valid_info[4],axis=1),valid_labels)\n",
    "        save_preds = valid_df[pred_cols].values \n",
    "        elapsed = time.time() - start_time\n",
    "\n",
    "        LOGGER.info(f\"Epoch:{epoch} > Valid Loss: {(valid_info[0]):.04f}, top(1): {(valid_info[1]):.04f},top(2): {(valid_info[2]):.04f}, top(5): {(valid_info[3]):.04f}, time: {elapsed:.0f}s \")\n",
    "    \n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            LOGGER.info(f'Epoch {epoch} - Save Best Score: {best_score:.4f} Model')\n",
    "            torch.save({'model': model.state_dict(),\n",
    "                        'predictions': save_preds},\n",
    "                        OUTPUT_DIR+f\"asl_model_fold{fold}.pth\")\n",
    "                                        \n",
    "    predictions = torch.load(OUTPUT_DIR+f\"asl_model_fold{fold}.pth\", \n",
    "                             map_location=torch.device('cpu'))['predictions']\n",
    "    \n",
    "    valid_df['pred_label'] = predictions\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    return valid_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "========== fold: 0 training ==========\n",
      "========== Epoch: 0 training ==========\n",
      "Epoch:0 > Train Loss: 5.3758, Train Acc: 0.2016\n",
      "Epoch:0 > Valid Loss: 3.6831, top(1): 0.4141,top(2): 0.5595, top(5): 0.7255, time: 86s \n",
      "Epoch 0 - Save Best Score: 0.4141 Model\n",
      "========== Epoch: 1 training ==========\n",
      "Epoch:1 > Train Loss: 5.1370, Train Acc: 0.5305\n",
      "Epoch:1 > Valid Loss: 3.0141, top(1): 0.5788,top(2): 0.7079, top(5): 0.8288, time: 88s \n",
      "Epoch 1 - Save Best Score: 0.5788 Model\n",
      "========== Epoch: 2 training ==========\n",
      "Epoch:2 > Train Loss: 5.0393, Train Acc: 0.6387\n",
      "Epoch:2 > Valid Loss: 2.8763, top(1): 0.6130,top(2): 0.7395, top(5): 0.8578, time: 86s \n",
      "Epoch 2 - Save Best Score: 0.6128 Model\n",
      "========== Epoch: 3 training ==========\n",
      "Epoch:3 > Train Loss: 4.9923, Train Acc: 0.6921\n",
      "Epoch:3 > Valid Loss: 2.6777, top(1): 0.6366,top(2): 0.7657, top(5): 0.8737, time: 87s \n",
      "Epoch 3 - Save Best Score: 0.6362 Model\n",
      "========== Epoch: 4 training ==========\n",
      "Epoch:4 > Train Loss: 4.9647, Train Acc: 0.7230\n",
      "Epoch:4 > Valid Loss: 2.7192, top(1): 0.6641,top(2): 0.7788, top(5): 0.8784, time: 86s \n",
      "Epoch 4 - Save Best Score: 0.6643 Model\n",
      "========== Epoch: 5 training ==========\n",
      "Epoch:5 > Train Loss: 4.9421, Train Acc: 0.7504\n",
      "Epoch:5 > Valid Loss: 2.5910, top(1): 0.6845,top(2): 0.7998, top(5): 0.8937, time: 87s \n",
      "Epoch 5 - Save Best Score: 0.6849 Model\n",
      "========== Epoch: 6 training ==========\n",
      "Epoch:6 > Train Loss: 4.9264, Train Acc: 0.7715\n",
      "Epoch:6 > Valid Loss: 2.6096, top(1): 0.6718,top(2): 0.7981, top(5): 0.8890, time: 87s \n",
      "========== Epoch: 7 training ==========\n",
      "Epoch:7 > Train Loss: 4.9120, Train Acc: 0.7877\n",
      "Epoch:7 > Valid Loss: 2.5876, top(1): 0.6864,top(2): 0.8022, top(5): 0.8935, time: 86s \n",
      "Epoch 7 - Save Best Score: 0.6864 Model\n",
      "========== Epoch: 8 training ==========\n",
      "Epoch:8 > Train Loss: 4.9006, Train Acc: 0.8026\n",
      "Epoch:8 > Valid Loss: 2.5390, top(1): 0.7068,top(2): 0.8131, top(5): 0.8950, time: 86s \n",
      "Epoch 8 - Save Best Score: 0.7068 Model\n",
      "========== Epoch: 9 training ==========\n",
      "Epoch:9 > Train Loss: 4.8903, Train Acc: 0.8160\n",
      "Epoch:9 > Valid Loss: 2.5210, top(1): 0.7040,top(2): 0.8189, top(5): 0.9029, time: 86s \n",
      "========== Epoch: 10 training ==========\n",
      "Epoch:10 > Train Loss: 4.8805, Train Acc: 0.8281\n",
      "Epoch:10 > Valid Loss: 2.6074, top(1): 0.7165,top(2): 0.8185, top(5): 0.8999, time: 86s \n",
      "Epoch 10 - Save Best Score: 0.7167 Model\n",
      "========== Epoch: 11 training ==========\n",
      "Epoch:11 > Train Loss: 4.8718, Train Acc: 0.8382\n",
      "Epoch:11 > Valid Loss: 2.5503, top(1): 0.7043,top(2): 0.8056, top(5): 0.8905, time: 87s \n",
      "========== Epoch: 12 training ==========\n",
      "Epoch:12 > Train Loss: 4.8645, Train Acc: 0.8477\n",
      "Epoch:12 > Valid Loss: 2.4370, top(1): 0.7257,top(2): 0.8290, top(5): 0.9074, time: 87s \n",
      "Epoch 12 - Save Best Score: 0.7257 Model\n",
      "========== Epoch: 13 training ==========\n",
      "Epoch:13 > Train Loss: 4.8583, Train Acc: 0.8566\n",
      "Epoch:13 > Valid Loss: 2.5667, top(1): 0.7146,top(2): 0.8164, top(5): 0.8978, time: 86s \n",
      "========== Epoch: 14 training ==========\n",
      "Epoch:14 > Train Loss: 4.8505, Train Acc: 0.8661\n",
      "Epoch:14 > Valid Loss: 2.5030, top(1): 0.7268,top(2): 0.8308, top(5): 0.9076, time: 86s \n",
      "Epoch 14 - Save Best Score: 0.7268 Model\n",
      "========== Epoch: 15 training ==========\n",
      "Epoch:15 > Train Loss: 5.0125, Train Acc: 0.7868\n",
      "Epoch:15 > Valid Loss: 2.6497, top(1): 0.7180,top(2): 0.8269, top(5): 0.9027, time: 86s \n",
      "========== Epoch: 16 training ==========\n",
      "Epoch:16 > Train Loss: 4.9626, Train Acc: 0.8094\n",
      "Epoch:16 > Valid Loss: 2.7247, top(1): 0.7034,top(2): 0.8121, top(5): 0.8988, time: 86s \n",
      "========== Epoch: 17 training ==========\n",
      "Epoch:17 > Train Loss: 4.9487, Train Acc: 0.8143\n",
      "Epoch:17 > Valid Loss: 2.7045, top(1): 0.7081,top(2): 0.8153, top(5): 0.9040, time: 86s \n",
      "========== Epoch: 18 training ==========\n",
      "Epoch:18 > Train Loss: 4.9370, Train Acc: 0.8250\n",
      "Epoch:18 > Valid Loss: 2.6797, top(1): 0.7070,top(2): 0.8134, top(5): 0.8986, time: 88s \n",
      "========== Epoch: 19 training ==========\n",
      "Epoch:19 > Train Loss: 4.9301, Train Acc: 0.8318\n",
      "Epoch:19 > Valid Loss: 2.6775, top(1): 0.7135,top(2): 0.8222, top(5): 0.9102, time: 88s \n",
      "========== Epoch: 20 training ==========\n",
      "Epoch:20 > Train Loss: 4.9233, Train Acc: 0.8382\n",
      "Epoch:20 > Valid Loss: 2.5890, top(1): 0.7253,top(2): 0.8305, top(5): 0.9094, time: 86s \n",
      "========== Epoch: 21 training ==========\n",
      "Epoch:21 > Train Loss: 4.9177, Train Acc: 0.8443\n",
      "Epoch:21 > Valid Loss: 2.6255, top(1): 0.7206,top(2): 0.8316, top(5): 0.9128, time: 88s \n",
      "========== Epoch: 22 training ==========\n",
      "Epoch:22 > Train Loss: 4.9126, Train Acc: 0.8474\n",
      "Epoch:22 > Valid Loss: 2.5995, top(1): 0.7315,top(2): 0.8381, top(5): 0.9134, time: 88s \n",
      "Epoch 22 - Save Best Score: 0.7322 Model\n",
      "========== Epoch: 23 training ==========\n",
      "Epoch:23 > Train Loss: 4.9070, Train Acc: 0.8533\n",
      "Epoch:23 > Valid Loss: 2.6431, top(1): 0.7281,top(2): 0.8333, top(5): 0.9126, time: 87s \n",
      "========== Epoch: 24 training ==========\n",
      "Epoch:24 > Train Loss: 4.9027, Train Acc: 0.8581\n",
      "Epoch:24 > Valid Loss: 2.6408, top(1): 0.7247,top(2): 0.8329, top(5): 0.9132, time: 87s \n",
      "========== Epoch: 25 training ==========\n",
      "Epoch:25 > Train Loss: 4.8984, Train Acc: 0.8634\n",
      "Epoch:25 > Valid Loss: 2.6640, top(1): 0.7268,top(2): 0.8273, top(5): 0.9102, time: 87s \n",
      "========== Epoch: 26 training ==========\n",
      "Epoch:26 > Train Loss: 4.8932, Train Acc: 0.8680\n",
      "Epoch:26 > Valid Loss: 2.6727, top(1): 0.7285,top(2): 0.8301, top(5): 0.9119, time: 87s \n",
      "========== Epoch: 27 training ==========\n",
      "Epoch:27 > Train Loss: 4.8893, Train Acc: 0.8725\n",
      "Epoch:27 > Valid Loss: 2.6687, top(1): 0.7296,top(2): 0.8335, top(5): 0.9070, time: 88s \n",
      "========== Epoch: 28 training ==========\n",
      "Epoch:28 > Train Loss: 4.8857, Train Acc: 0.8770\n",
      "Epoch:28 > Valid Loss: 2.6388, top(1): 0.7313,top(2): 0.8370, top(5): 0.9098, time: 86s \n",
      "========== Epoch: 29 training ==========\n",
      "Epoch:29 > Train Loss: 4.8816, Train Acc: 0.8807\n",
      "Epoch:29 > Valid Loss: 2.6948, top(1): 0.7247,top(2): 0.8308, top(5): 0.9126, time: 87s \n",
      "========== Epoch: 30 training ==========\n",
      "Epoch:30 > Train Loss: 4.8786, Train Acc: 0.8836\n",
      "Epoch:30 > Valid Loss: 2.6144, top(1): 0.7311,top(2): 0.8366, top(5): 0.9141, time: 86s \n",
      "========== Epoch: 31 training ==========\n",
      "Epoch:31 > Train Loss: 4.8747, Train Acc: 0.8885\n",
      "Epoch:31 > Valid Loss: 2.6426, top(1): 0.7285,top(2): 0.8363, top(5): 0.9137, time: 87s \n",
      "========== Epoch: 32 training ==========\n",
      "Epoch:32 > Train Loss: 4.8710, Train Acc: 0.8919\n",
      "Epoch:32 > Valid Loss: 2.5931, top(1): 0.7375,top(2): 0.8424, top(5): 0.9160, time: 87s \n",
      "Epoch 32 - Save Best Score: 0.7375 Model\n",
      "========== Epoch: 33 training ==========\n",
      "Epoch:33 > Train Loss: 4.8681, Train Acc: 0.8959\n",
      "Epoch:33 > Valid Loss: 2.6620, top(1): 0.7292,top(2): 0.8366, top(5): 0.9130, time: 87s \n",
      "========== Epoch: 34 training ==========\n",
      "Epoch:34 > Train Loss: 4.8653, Train Acc: 0.8995\n",
      "Epoch:34 > Valid Loss: 2.6482, top(1): 0.7360,top(2): 0.8378, top(5): 0.9167, time: 85s \n",
      "========== Epoch: 35 training ==========\n",
      "Epoch:35 > Train Loss: 4.8628, Train Acc: 0.9022\n",
      "Epoch:35 > Valid Loss: 2.6319, top(1): 0.7354,top(2): 0.8402, top(5): 0.9165, time: 84s \n",
      "========== Epoch: 36 training ==========\n",
      "Epoch:36 > Train Loss: 4.8173, Train Acc: 0.9174\n",
      "Epoch:36 > Valid Loss: 2.5571, top(1): 0.7390,top(2): 0.8426, top(5): 0.9158, time: 87s \n",
      "Epoch 36 - Save Best Score: 0.7395 Model\n",
      "========== Epoch: 37 training ==========\n",
      "Epoch:37 > Train Loss: 4.8134, Train Acc: 0.9217\n",
      "Epoch:37 > Valid Loss: 2.5641, top(1): 0.7352,top(2): 0.8400, top(5): 0.9141, time: 87s \n",
      "========== Epoch: 38 training ==========\n",
      "Epoch:38 > Train Loss: 4.8109, Train Acc: 0.9243\n",
      "Epoch:38 > Valid Loss: 2.5564, top(1): 0.7388,top(2): 0.8426, top(5): 0.9145, time: 87s \n",
      "========== Epoch: 39 training ==========\n",
      "Epoch:39 > Train Loss: 4.8084, Train Acc: 0.9271\n",
      "Epoch:39 > Valid Loss: 2.5674, top(1): 0.7378,top(2): 0.8445, top(5): 0.9134, time: 86s \n",
      "========== Epoch: 40 training ==========\n",
      "Epoch:40 > Train Loss: 4.8065, Train Acc: 0.9304\n",
      "Epoch:40 > Valid Loss: 2.5697, top(1): 0.7384,top(2): 0.8409, top(5): 0.9152, time: 87s \n",
      "========== Epoch: 41 training ==========\n",
      "Epoch:41 > Train Loss: 4.8050, Train Acc: 0.9309\n",
      "Epoch:41 > Valid Loss: 2.5505, top(1): 0.7418,top(2): 0.8451, top(5): 0.9173, time: 87s \n",
      "Epoch 41 - Save Best Score: 0.7414 Model\n",
      "========== Epoch: 42 training ==========\n",
      "Epoch:42 > Train Loss: 4.8034, Train Acc: 0.9330\n",
      "Epoch:42 > Valid Loss: 2.5709, top(1): 0.7425,top(2): 0.8462, top(5): 0.9149, time: 86s \n",
      "Epoch 42 - Save Best Score: 0.7425 Model\n",
      "========== Epoch: 43 training ==========\n",
      "Epoch:43 > Train Loss: 4.8024, Train Acc: 0.9343\n",
      "Epoch:43 > Valid Loss: 2.5583, top(1): 0.7367,top(2): 0.8443, top(5): 0.9160, time: 86s \n",
      "========== Epoch: 44 training ==========\n",
      "Epoch:44 > Train Loss: 4.8014, Train Acc: 0.9348\n",
      "Epoch:44 > Valid Loss: 2.5647, top(1): 0.7356,top(2): 0.8436, top(5): 0.9165, time: 87s \n",
      "========== Epoch: 45 training ==========\n",
      "Epoch:45 > Train Loss: 4.8008, Train Acc: 0.9359\n",
      "Epoch:45 > Valid Loss: 2.5704, top(1): 0.7360,top(2): 0.8460, top(5): 0.9167, time: 87s \n",
      "========== Epoch: 46 training ==========\n",
      "Epoch:46 > Train Loss: 4.7998, Train Acc: 0.9366\n",
      "Epoch:46 > Valid Loss: 2.5684, top(1): 0.7371,top(2): 0.8445, top(5): 0.9165, time: 86s \n",
      "========== Epoch: 47 training ==========\n",
      "Epoch:47 > Train Loss: 4.7995, Train Acc: 0.9369\n",
      "Epoch:47 > Valid Loss: 2.5683, top(1): 0.7363,top(2): 0.8454, top(5): 0.9167, time: 85s \n",
      "========== Epoch: 48 training ==========\n",
      "Epoch:48 > Train Loss: 4.7990, Train Acc: 0.9381\n",
      "Epoch:48 > Valid Loss: 2.5677, top(1): 0.7352,top(2): 0.8460, top(5): 0.9169, time: 86s \n",
      "========== Epoch: 49 training ==========\n",
      "Epoch:49 > Train Loss: 4.7989, Train Acc: 0.9383\n",
      "Epoch:49 > Valid Loss: 2.5686, top(1): 0.7363,top(2): 0.8456, top(5): 0.9160, time: 87s \n",
      "========== fold: 0 result ==========\n",
      "Score: 0.7425\n",
      "========== fold: 4 training ==========\n",
      "========== Epoch: 0 training ==========\n",
      "Epoch:0 > Train Loss: 5.3787, Train Acc: 0.1971\n",
      "Epoch:0 > Valid Loss: 3.8712, top(1): 0.3695,top(2): 0.5120, top(5): 0.6786, time: 86s \n",
      "Epoch 0 - Save Best Score: 0.3695 Model\n",
      "========== Epoch: 1 training ==========\n",
      "Epoch:1 > Train Loss: 5.1408, Train Acc: 0.5259\n",
      "Epoch:1 > Valid Loss: 3.0932, top(1): 0.5568,top(2): 0.6939, top(5): 0.8270, time: 87s \n",
      "Epoch 1 - Save Best Score: 0.5568 Model\n",
      "========== Epoch: 2 training ==========\n",
      "Epoch:2 > Train Loss: 5.0441, Train Acc: 0.6378\n",
      "Epoch:2 > Valid Loss: 2.8533, top(1): 0.6310,top(2): 0.7433, top(5): 0.8562, time: 87s \n",
      "Epoch 2 - Save Best Score: 0.6305 Model\n",
      "========== Epoch: 3 training ==========\n",
      "Epoch:3 > Train Loss: 4.9960, Train Acc: 0.6915\n",
      "Epoch:3 > Valid Loss: 2.8900, top(1): 0.6511,top(2): 0.7741, top(5): 0.8809, time: 87s \n",
      "Epoch 3 - Save Best Score: 0.6513 Model\n",
      "========== Epoch: 4 training ==========\n",
      "Epoch:4 > Train Loss: 4.9678, Train Acc: 0.7241\n",
      "Epoch:4 > Valid Loss: 2.7574, top(1): 0.6651,top(2): 0.7930, top(5): 0.8966, time: 88s \n",
      "Epoch 4 - Save Best Score: 0.6651 Model\n",
      "========== Epoch: 5 training ==========\n",
      "Epoch:5 > Train Loss: 4.9460, Train Acc: 0.7494\n",
      "Epoch:5 > Valid Loss: 2.6652, top(1): 0.7035,top(2): 0.8100, top(5): 0.9010, time: 86s \n",
      "Epoch 5 - Save Best Score: 0.7035 Model\n",
      "========== Epoch: 6 training ==========\n",
      "Epoch:6 > Train Loss: 4.9292, Train Acc: 0.7713\n",
      "Epoch:6 > Valid Loss: 2.5799, top(1): 0.7072,top(2): 0.8199, top(5): 0.9084, time: 88s \n",
      "Epoch 6 - Save Best Score: 0.7072 Model\n",
      "========== Epoch: 7 training ==========\n",
      "Epoch:7 > Train Loss: 4.9143, Train Acc: 0.7883\n",
      "Epoch:7 > Valid Loss: 2.5429, top(1): 0.7219,top(2): 0.8270, top(5): 0.9076, time: 88s \n",
      "Epoch 7 - Save Best Score: 0.7217 Model\n",
      "========== Epoch: 8 training ==========\n",
      "Epoch:8 > Train Loss: 4.9029, Train Acc: 0.8032\n",
      "Epoch:8 > Valid Loss: 2.5212, top(1): 0.7188,top(2): 0.8230, top(5): 0.9076, time: 87s \n",
      "========== Epoch: 9 training ==========\n",
      "Epoch:9 > Train Loss: 4.8939, Train Acc: 0.8142\n",
      "Epoch:9 > Valid Loss: 2.5750, top(1): 0.7356,top(2): 0.8361, top(5): 0.9068, time: 87s \n",
      "Epoch 9 - Save Best Score: 0.7352 Model\n",
      "========== Epoch: 10 training ==========\n",
      "Epoch:10 > Train Loss: 4.8831, Train Acc: 0.8287\n",
      "Epoch:10 > Valid Loss: 2.4548, top(1): 0.7368,top(2): 0.8378, top(5): 0.9167, time: 87s \n",
      "Epoch 10 - Save Best Score: 0.7368 Model\n",
      "========== Epoch: 11 training ==========\n",
      "Epoch:11 > Train Loss: 4.8734, Train Acc: 0.8399\n",
      "Epoch:11 > Valid Loss: 2.6060, top(1): 0.7286,top(2): 0.8357, top(5): 0.9136, time: 87s \n",
      "========== Epoch: 12 training ==========\n",
      "Epoch:12 > Train Loss: 4.8662, Train Acc: 0.8498\n",
      "Epoch:12 > Valid Loss: 2.6068, top(1): 0.7244,top(2): 0.8195, top(5): 0.9053, time: 88s \n",
      "========== Epoch: 13 training ==========\n",
      "Epoch:13 > Train Loss: 4.8588, Train Acc: 0.8589\n",
      "Epoch:13 > Valid Loss: 2.5468, top(1): 0.7447,top(2): 0.8407, top(5): 0.9134, time: 87s \n",
      "Epoch 13 - Save Best Score: 0.7447 Model\n",
      "========== Epoch: 14 training ==========\n",
      "Epoch:14 > Train Loss: 4.8516, Train Acc: 0.8687\n",
      "Epoch:14 > Valid Loss: 2.5206, top(1): 0.7379,top(2): 0.8334, top(5): 0.9173, time: 87s \n",
      "========== Epoch: 15 training ==========\n",
      "Epoch:15 > Train Loss: 5.0146, Train Acc: 0.7842\n",
      "Epoch:15 > Valid Loss: 2.6904, top(1): 0.7383,top(2): 0.8380, top(5): 0.9157, time: 88s \n",
      "========== Epoch: 16 training ==========\n",
      "Epoch:16 > Train Loss: 4.9662, Train Acc: 0.8077\n",
      "Epoch:16 > Valid Loss: 2.6936, top(1): 0.7319,top(2): 0.8332, top(5): 0.9155, time: 88s \n",
      "========== Epoch: 17 training ==========\n",
      "Epoch:17 > Train Loss: 4.9507, Train Acc: 0.8162\n",
      "Epoch:17 > Valid Loss: 2.6120, top(1): 0.7406,top(2): 0.8375, top(5): 0.9159, time: 85s \n",
      "========== Epoch: 18 training ==========\n",
      "Epoch:18 > Train Loss: 4.9400, Train Acc: 0.8257\n",
      "Epoch:18 > Valid Loss: 2.6135, top(1): 0.7368,top(2): 0.8419, top(5): 0.9167, time: 86s \n",
      "========== Epoch: 19 training ==========\n",
      "Epoch:19 > Train Loss: 4.9316, Train Acc: 0.8337\n",
      "Epoch:19 > Valid Loss: 2.6138, top(1): 0.7571,top(2): 0.8558, top(5): 0.9250, time: 86s \n",
      "Epoch 19 - Save Best Score: 0.7571 Model\n",
      "========== Epoch: 20 training ==========\n",
      "Epoch:20 > Train Loss: 4.9266, Train Acc: 0.8367\n",
      "Epoch:20 > Valid Loss: 2.6605, top(1): 0.7484,top(2): 0.8417, top(5): 0.9134, time: 84s \n",
      "========== Epoch: 21 training ==========\n",
      "Epoch:21 > Train Loss: 4.9202, Train Acc: 0.8436\n",
      "Epoch:21 > Valid Loss: 2.6053, top(1): 0.7458,top(2): 0.8454, top(5): 0.9182, time: 85s \n",
      "========== Epoch: 22 training ==========\n",
      "Epoch:22 > Train Loss: 4.9136, Train Acc: 0.8487\n",
      "Epoch:22 > Valid Loss: 2.6856, top(1): 0.7368,top(2): 0.8404, top(5): 0.9132, time: 86s \n",
      "========== Epoch: 23 training ==========\n",
      "Epoch:23 > Train Loss: 4.9093, Train Acc: 0.8551\n",
      "Epoch:23 > Valid Loss: 2.6137, top(1): 0.7534,top(2): 0.8487, top(5): 0.9240, time: 86s \n",
      "========== Epoch: 24 training ==========\n",
      "Epoch:24 > Train Loss: 4.9053, Train Acc: 0.8587\n",
      "Epoch:24 > Valid Loss: 2.6190, top(1): 0.7627,top(2): 0.8593, top(5): 0.9269, time: 85s \n",
      "Epoch 24 - Save Best Score: 0.7630 Model\n",
      "========== Epoch: 25 training ==========\n",
      "Epoch:25 > Train Loss: 4.8998, Train Acc: 0.8649\n",
      "Epoch:25 > Valid Loss: 2.6407, top(1): 0.7594,top(2): 0.8547, top(5): 0.9215, time: 84s \n",
      "========== Epoch: 26 training ==========\n",
      "Epoch:26 > Train Loss: 4.8949, Train Acc: 0.8699\n",
      "Epoch:26 > Valid Loss: 2.5918, top(1): 0.7646,top(2): 0.8634, top(5): 0.9287, time: 86s \n",
      "Epoch 26 - Save Best Score: 0.7648 Model\n",
      "========== Epoch: 27 training ==========\n",
      "Epoch:27 > Train Loss: 4.8918, Train Acc: 0.8735\n",
      "Epoch:27 > Valid Loss: 2.6437, top(1): 0.7526,top(2): 0.8494, top(5): 0.9198, time: 85s \n",
      "========== Epoch: 28 training ==========\n",
      "Epoch:28 > Train Loss: 4.8879, Train Acc: 0.8777\n",
      "Epoch:28 > Valid Loss: 2.6288, top(1): 0.7677,top(2): 0.8655, top(5): 0.9250, time: 86s \n",
      "Epoch 28 - Save Best Score: 0.7677 Model\n",
      "========== Epoch: 29 training ==========\n",
      "Epoch:29 > Train Loss: 4.8833, Train Acc: 0.8826\n",
      "Epoch:29 > Valid Loss: 2.6540, top(1): 0.7540,top(2): 0.8516, top(5): 0.9179, time: 86s \n",
      "========== Epoch: 30 training ==========\n",
      "Epoch:30 > Train Loss: 4.8803, Train Acc: 0.8856\n",
      "Epoch:30 > Valid Loss: 2.6430, top(1): 0.7592,top(2): 0.8543, top(5): 0.9233, time: 84s \n",
      "========== Epoch: 31 training ==========\n",
      "Epoch:31 > Train Loss: 4.8765, Train Acc: 0.8897\n",
      "Epoch:31 > Valid Loss: 2.6385, top(1): 0.7536,top(2): 0.8529, top(5): 0.9198, time: 86s \n",
      "========== Epoch: 32 training ==========\n",
      "Epoch:32 > Train Loss: 4.8726, Train Acc: 0.8948\n",
      "Epoch:32 > Valid Loss: 2.6553, top(1): 0.7551,top(2): 0.8514, top(5): 0.9202, time: 86s \n",
      "========== Epoch: 33 training ==========\n",
      "Epoch:33 > Train Loss: 4.8700, Train Acc: 0.8981\n",
      "Epoch:33 > Valid Loss: 2.6212, top(1): 0.7561,top(2): 0.8562, top(5): 0.9260, time: 86s \n",
      "========== Epoch: 34 training ==========\n",
      "Epoch:34 > Train Loss: 4.8672, Train Acc: 0.9010\n",
      "Epoch:34 > Valid Loss: 2.6182, top(1): 0.7578,top(2): 0.8552, top(5): 0.9198, time: 86s \n",
      "========== Epoch: 35 training ==========\n",
      "Epoch:35 > Train Loss: 4.8641, Train Acc: 0.9047\n",
      "Epoch:35 > Valid Loss: 2.6354, top(1): 0.7571,top(2): 0.8541, top(5): 0.9244, time: 84s \n",
      "========== Epoch: 36 training ==========\n",
      "Epoch:36 > Train Loss: 4.8183, Train Acc: 0.9202\n",
      "Epoch:36 > Valid Loss: 2.5697, top(1): 0.7615,top(2): 0.8589, top(5): 0.9279, time: 86s \n",
      "========== Epoch: 37 training ==========\n",
      "Epoch:37 > Train Loss: 4.8144, Train Acc: 0.9236\n",
      "Epoch:37 > Valid Loss: 2.5694, top(1): 0.7623,top(2): 0.8574, top(5): 0.9254, time: 85s \n",
      "========== Epoch: 38 training ==========\n",
      "Epoch:38 > Train Loss: 4.8117, Train Acc: 0.9272\n",
      "Epoch:38 > Valid Loss: 2.5684, top(1): 0.7654,top(2): 0.8593, top(5): 0.9242, time: 85s \n",
      "========== Epoch: 39 training ==========\n",
      "Epoch:39 > Train Loss: 4.8096, Train Acc: 0.9293\n",
      "Epoch:39 > Valid Loss: 2.5748, top(1): 0.7650,top(2): 0.8595, top(5): 0.9235, time: 85s \n",
      "========== Epoch: 40 training ==========\n",
      "Epoch:40 > Train Loss: 4.8078, Train Acc: 0.9316\n",
      "Epoch:40 > Valid Loss: 2.5839, top(1): 0.7584,top(2): 0.8568, top(5): 0.9256, time: 85s \n",
      "========== Epoch: 41 training ==========\n",
      "Epoch:41 > Train Loss: 4.8063, Train Acc: 0.9326\n",
      "Epoch:41 > Valid Loss: 2.5813, top(1): 0.7661,top(2): 0.8583, top(5): 0.9248, time: 86s \n",
      "========== Epoch: 42 training ==========\n",
      "Epoch:42 > Train Loss: 4.8044, Train Acc: 0.9354\n",
      "Epoch:42 > Valid Loss: 2.5955, top(1): 0.7659,top(2): 0.8566, top(5): 0.9252, time: 85s \n",
      "========== Epoch: 43 training ==========\n",
      "Epoch:43 > Train Loss: 4.8032, Train Acc: 0.9369\n",
      "Epoch:43 > Valid Loss: 2.5672, top(1): 0.7654,top(2): 0.8585, top(5): 0.9246, time: 85s \n",
      "========== Epoch: 44 training ==========\n",
      "Epoch:44 > Train Loss: 4.8024, Train Acc: 0.9375\n",
      "Epoch:44 > Valid Loss: 2.5682, top(1): 0.7665,top(2): 0.8585, top(5): 0.9260, time: 86s \n",
      "========== Epoch: 45 training ==========\n",
      "Epoch:45 > Train Loss: 4.8015, Train Acc: 0.9380\n",
      "Epoch:45 > Valid Loss: 2.5745, top(1): 0.7636,top(2): 0.8576, top(5): 0.9248, time: 85s \n",
      "========== Epoch: 46 training ==========\n",
      "Epoch:46 > Train Loss: 4.8008, Train Acc: 0.9386\n",
      "Epoch:46 > Valid Loss: 2.5818, top(1): 0.7648,top(2): 0.8589, top(5): 0.9248, time: 86s \n",
      "========== Epoch: 47 training ==========\n",
      "Epoch:47 > Train Loss: 4.8003, Train Acc: 0.9392\n",
      "Epoch:47 > Valid Loss: 2.5827, top(1): 0.7650,top(2): 0.8589, top(5): 0.9252, time: 85s \n",
      "========== Epoch: 48 training ==========\n",
      "Epoch:48 > Train Loss: 4.8003, Train Acc: 0.9396\n",
      "Epoch:48 > Valid Loss: 2.5819, top(1): 0.7652,top(2): 0.8576, top(5): 0.9254, time: 86s \n",
      "========== Epoch: 49 training ==========\n",
      "Epoch:49 > Train Loss: 4.7999, Train Acc: 0.9401\n",
      "Epoch:49 > Valid Loss: 2.5810, top(1): 0.7652,top(2): 0.8585, top(5): 0.9252, time: 85s \n",
      "========== fold: 4 result ==========\n",
      "Score: 0.7677\n",
      "========== fold: 8 training ==========\n",
      "========== Epoch: 0 training ==========\n",
      "Epoch:0 > Train Loss: 5.3842, Train Acc: 0.1916\n",
      "Epoch:0 > Valid Loss: 3.6410, top(1): 0.4473,top(2): 0.6156, top(5): 0.7852, time: 85s \n",
      "Epoch 0 - Save Best Score: 0.4473 Model\n",
      "========== Epoch: 1 training ==========\n",
      "Epoch:1 > Train Loss: 5.1440, Train Acc: 0.5233\n",
      "Epoch:1 > Valid Loss: 3.0562, top(1): 0.5880,top(2): 0.7436, top(5): 0.8618, time: 84s \n",
      "Epoch 1 - Save Best Score: 0.5882 Model\n",
      "========== Epoch: 2 training ==========\n",
      "Epoch:2 > Train Loss: 5.0458, Train Acc: 0.6349\n",
      "Epoch:2 > Valid Loss: 2.7089, top(1): 0.6688,top(2): 0.8087, top(5): 0.9049, time: 84s \n",
      "Epoch 2 - Save Best Score: 0.6688 Model\n",
      "========== Epoch: 3 training ==========\n",
      "Epoch:3 > Train Loss: 4.9991, Train Acc: 0.6862\n",
      "Epoch:3 > Valid Loss: 2.6044, top(1): 0.6821,top(2): 0.8227, top(5): 0.9143, time: 86s \n",
      "Epoch 3 - Save Best Score: 0.6817 Model\n",
      "========== Epoch: 4 training ==========\n",
      "Epoch:4 > Train Loss: 4.9677, Train Acc: 0.7218\n",
      "Epoch:4 > Valid Loss: 2.5294, top(1): 0.7378,top(2): 0.8651, top(5): 0.9381, time: 83s \n",
      "Epoch 4 - Save Best Score: 0.7376 Model\n",
      "========== Epoch: 5 training ==========\n",
      "Epoch:5 > Train Loss: 4.9460, Train Acc: 0.7467\n",
      "Epoch:5 > Valid Loss: 2.4407, top(1): 0.7371,top(2): 0.8509, top(5): 0.9241, time: 85s \n",
      "========== Epoch: 6 training ==========\n",
      "Epoch:6 > Train Loss: 4.9297, Train Acc: 0.7659\n",
      "Epoch:6 > Valid Loss: 2.3885, top(1): 0.7497,top(2): 0.8653, top(5): 0.9381, time: 84s \n",
      "Epoch 6 - Save Best Score: 0.7495 Model\n",
      "========== Epoch: 7 training ==========\n",
      "Epoch:7 > Train Loss: 4.9160, Train Acc: 0.7831\n",
      "Epoch:7 > Valid Loss: 2.4375, top(1): 0.7606,top(2): 0.8699, top(5): 0.9375, time: 85s \n",
      "Epoch 7 - Save Best Score: 0.7606 Model\n",
      "========== Epoch: 8 training ==========\n",
      "Epoch:8 > Train Loss: 4.9023, Train Acc: 0.8012\n",
      "Epoch:8 > Valid Loss: 2.4371, top(1): 0.7597,top(2): 0.8630, top(5): 0.9362, time: 84s \n",
      "========== Epoch: 9 training ==========\n",
      "Epoch:9 > Train Loss: 4.8924, Train Acc: 0.8125\n",
      "Epoch:9 > Valid Loss: 2.3771, top(1): 0.7564,top(2): 0.8685, top(5): 0.9377, time: 85s \n",
      "========== Epoch: 10 training ==========\n",
      "Epoch:10 > Train Loss: 4.8834, Train Acc: 0.8249\n",
      "Epoch:10 > Valid Loss: 2.3505, top(1): 0.7528,top(2): 0.8668, top(5): 0.9412, time: 85s \n",
      "========== Epoch: 11 training ==========\n",
      "Epoch:11 > Train Loss: 4.8749, Train Acc: 0.8358\n",
      "Epoch:11 > Valid Loss: 2.3942, top(1): 0.7664,top(2): 0.8693, top(5): 0.9356, time: 85s \n",
      "Epoch 11 - Save Best Score: 0.7666 Model\n",
      "========== Epoch: 12 training ==========\n",
      "Epoch:12 > Train Loss: 4.8664, Train Acc: 0.8456\n",
      "Epoch:12 > Valid Loss: 2.3205, top(1): 0.7838,top(2): 0.8894, top(5): 0.9473, time: 85s \n",
      "Epoch 12 - Save Best Score: 0.7838 Model\n",
      "========== Epoch: 13 training ==========\n",
      "Epoch:13 > Train Loss: 4.8585, Train Acc: 0.8560\n",
      "Epoch:13 > Valid Loss: 2.3243, top(1): 0.7742,top(2): 0.8793, top(5): 0.9402, time: 85s \n",
      "========== Epoch: 14 training ==========\n",
      "Epoch:14 > Train Loss: 4.8516, Train Acc: 0.8651\n",
      "Epoch:14 > Valid Loss: 2.2568, top(1): 0.7890,top(2): 0.8946, top(5): 0.9448, time: 85s \n",
      "Epoch 14 - Save Best Score: 0.7890 Model\n",
      "========== Epoch: 15 training ==========\n",
      "Epoch:15 > Train Loss: 5.0132, Train Acc: 0.7836\n",
      "Epoch:15 > Valid Loss: 2.5386, top(1): 0.7631,top(2): 0.8802, top(5): 0.9421, time: 85s \n",
      "========== Epoch: 16 training ==========\n",
      "Epoch:16 > Train Loss: 4.9642, Train Acc: 0.8073\n",
      "Epoch:16 > Valid Loss: 2.4737, top(1): 0.7905,top(2): 0.8967, top(5): 0.9458, time: 85s \n",
      "Epoch 16 - Save Best Score: 0.7905 Model\n",
      "========== Epoch: 17 training ==========\n",
      "Epoch:17 > Train Loss: 4.9481, Train Acc: 0.8161\n",
      "Epoch:17 > Valid Loss: 2.5378, top(1): 0.7808,top(2): 0.8852, top(5): 0.9423, time: 85s \n",
      "========== Epoch: 18 training ==========\n",
      "Epoch:18 > Train Loss: 4.9382, Train Acc: 0.8242\n",
      "Epoch:18 > Valid Loss: 2.5021, top(1): 0.7737,top(2): 0.8887, top(5): 0.9442, time: 84s \n",
      "========== Epoch: 19 training ==========\n",
      "Epoch:19 > Train Loss: 4.9311, Train Acc: 0.8302\n",
      "Epoch:19 > Valid Loss: 2.4677, top(1): 0.7913,top(2): 0.8938, top(5): 0.9427, time: 84s \n",
      "Epoch 19 - Save Best Score: 0.7913 Model\n",
      "========== Epoch: 20 training ==========\n",
      "Epoch:20 > Train Loss: 4.9241, Train Acc: 0.8368\n",
      "Epoch:20 > Valid Loss: 2.4541, top(1): 0.7992,top(2): 0.8923, top(5): 0.9471, time: 85s \n",
      "Epoch 20 - Save Best Score: 0.7992 Model\n",
      "========== Epoch: 21 training ==========\n",
      "Epoch:21 > Train Loss: 4.9185, Train Acc: 0.8415\n",
      "Epoch:21 > Valid Loss: 2.4824, top(1): 0.7731,top(2): 0.8787, top(5): 0.9404, time: 85s \n",
      "========== Epoch: 22 training ==========\n",
      "Epoch:22 > Train Loss: 4.9130, Train Acc: 0.8469\n",
      "Epoch:22 > Valid Loss: 2.4830, top(1): 0.7788,top(2): 0.8816, top(5): 0.9421, time: 84s \n",
      "========== Epoch: 23 training ==========\n",
      "Epoch:23 > Train Loss: 4.9077, Train Acc: 0.8533\n",
      "Epoch:23 > Valid Loss: 2.4938, top(1): 0.7909,top(2): 0.8938, top(5): 0.9467, time: 84s \n",
      "========== Epoch: 24 training ==========\n",
      "Epoch:24 > Train Loss: 4.9032, Train Acc: 0.8569\n",
      "Epoch:24 > Valid Loss: 2.4564, top(1): 0.7915,top(2): 0.8887, top(5): 0.9523, time: 84s \n",
      "========== Epoch: 25 training ==========\n",
      "Epoch:25 > Train Loss: 4.8982, Train Acc: 0.8632\n",
      "Epoch:25 > Valid Loss: 2.4631, top(1): 0.7825,top(2): 0.8785, top(5): 0.9391, time: 86s \n",
      "========== Epoch: 26 training ==========\n",
      "Epoch:26 > Train Loss: 4.8938, Train Acc: 0.8671\n",
      "Epoch:26 > Valid Loss: 2.4582, top(1): 0.7806,top(2): 0.8875, top(5): 0.9456, time: 85s \n",
      "========== Epoch: 27 training ==========\n",
      "Epoch:27 > Train Loss: 4.8895, Train Acc: 0.8727\n",
      "Epoch:27 > Valid Loss: 2.4687, top(1): 0.7907,top(2): 0.8904, top(5): 0.9490, time: 85s \n",
      "========== Epoch: 28 training ==========\n",
      "Epoch:28 > Train Loss: 4.8858, Train Acc: 0.8770\n",
      "Epoch:28 > Valid Loss: 2.4153, top(1): 0.7957,top(2): 0.8921, top(5): 0.9494, time: 85s \n",
      "========== Epoch: 29 training ==========\n",
      "Epoch:29 > Train Loss: 4.8820, Train Acc: 0.8812\n",
      "Epoch:29 > Valid Loss: 2.4328, top(1): 0.7926,top(2): 0.8921, top(5): 0.9471, time: 85s \n",
      "========== Epoch: 30 training ==========\n",
      "Epoch:30 > Train Loss: 4.8789, Train Acc: 0.8837\n",
      "Epoch:30 > Valid Loss: 2.4323, top(1): 0.7905,top(2): 0.8938, top(5): 0.9477, time: 85s \n",
      "========== Epoch: 31 training ==========\n",
      "Epoch:31 > Train Loss: 4.8744, Train Acc: 0.8899\n",
      "Epoch:31 > Valid Loss: 2.4481, top(1): 0.7911,top(2): 0.8927, top(5): 0.9511, time: 85s \n",
      "========== Epoch: 32 training ==========\n",
      "Epoch:32 > Train Loss: 4.8707, Train Acc: 0.8936\n",
      "Epoch:32 > Valid Loss: 2.4829, top(1): 0.7896,top(2): 0.8931, top(5): 0.9450, time: 84s \n",
      "========== Epoch: 33 training ==========\n",
      "Epoch:33 > Train Loss: 4.8678, Train Acc: 0.8969\n",
      "Epoch:33 > Valid Loss: 2.4329, top(1): 0.7890,top(2): 0.8906, top(5): 0.9488, time: 86s \n",
      "========== Epoch: 34 training ==========\n",
      "Epoch:34 > Train Loss: 4.8653, Train Acc: 0.9002\n",
      "Epoch:34 > Valid Loss: 2.4216, top(1): 0.8013,top(2): 0.8994, top(5): 0.9502, time: 84s \n",
      "Epoch 34 - Save Best Score: 0.8013 Model\n",
      "========== Epoch: 35 training ==========\n",
      "Epoch:35 > Train Loss: 4.8624, Train Acc: 0.9033\n",
      "Epoch:35 > Valid Loss: 2.4251, top(1): 0.7984,top(2): 0.8946, top(5): 0.9492, time: 85s \n",
      "========== Epoch: 36 training ==========\n",
      "Epoch:36 > Train Loss: 4.8173, Train Acc: 0.9180\n",
      "Epoch:36 > Valid Loss: 2.3741, top(1): 0.7978,top(2): 0.8927, top(5): 0.9475, time: 85s \n",
      "========== Epoch: 37 training ==========\n",
      "Epoch:37 > Train Loss: 4.8137, Train Acc: 0.9220\n",
      "Epoch:37 > Valid Loss: 2.3809, top(1): 0.7934,top(2): 0.8973, top(5): 0.9498, time: 86s \n",
      "========== Epoch: 38 training ==========\n",
      "Epoch:38 > Train Loss: 4.8112, Train Acc: 0.9248\n",
      "Epoch:38 > Valid Loss: 2.3481, top(1): 0.7978,top(2): 0.8967, top(5): 0.9490, time: 86s \n",
      "========== Epoch: 39 training ==========\n",
      "Epoch:39 > Train Loss: 4.8091, Train Acc: 0.9276\n",
      "Epoch:39 > Valid Loss: 2.3455, top(1): 0.8015,top(2): 0.8963, top(5): 0.9492, time: 84s \n",
      "========== Epoch: 40 training ==========\n",
      "Epoch:40 > Train Loss: 4.8069, Train Acc: 0.9300\n",
      "Epoch:40 > Valid Loss: 2.3427, top(1): 0.8026,top(2): 0.8973, top(5): 0.9511, time: 85s \n",
      "Epoch 40 - Save Best Score: 0.8028 Model\n",
      "========== Epoch: 41 training ==========\n",
      "Epoch:41 > Train Loss: 4.8053, Train Acc: 0.9321\n",
      "Epoch:41 > Valid Loss: 2.3451, top(1): 0.8028,top(2): 0.8954, top(5): 0.9496, time: 85s \n",
      "========== Epoch: 42 training ==========\n",
      "Epoch:42 > Train Loss: 4.8038, Train Acc: 0.9331\n",
      "Epoch:42 > Valid Loss: 2.3661, top(1): 0.8005,top(2): 0.8929, top(5): 0.9477, time: 85s \n",
      "========== Epoch: 43 training ==========\n",
      "Epoch:43 > Train Loss: 4.8026, Train Acc: 0.9340\n",
      "Epoch:43 > Valid Loss: 2.3693, top(1): 0.7969,top(2): 0.8931, top(5): 0.9477, time: 84s \n",
      "========== Epoch: 44 training ==========\n",
      "Epoch:44 > Train Loss: 4.8017, Train Acc: 0.9363\n",
      "Epoch:44 > Valid Loss: 2.3459, top(1): 0.8011,top(2): 0.8950, top(5): 0.9494, time: 85s \n",
      "========== Epoch: 45 training ==========\n",
      "Epoch:45 > Train Loss: 4.8007, Train Acc: 0.9364\n",
      "Epoch:45 > Valid Loss: 2.3680, top(1): 0.7967,top(2): 0.8934, top(5): 0.9479, time: 85s \n",
      "========== Epoch: 46 training ==========\n",
      "Epoch:46 > Train Loss: 4.8000, Train Acc: 0.9366\n",
      "Epoch:46 > Valid Loss: 2.3682, top(1): 0.7988,top(2): 0.8929, top(5): 0.9479, time: 85s \n",
      "========== Epoch: 47 training ==========\n",
      "Epoch:47 > Train Loss: 4.7998, Train Acc: 0.9376\n",
      "Epoch:47 > Valid Loss: 2.3592, top(1): 0.7980,top(2): 0.8938, top(5): 0.9475, time: 85s \n",
      "========== Epoch: 48 training ==========\n",
      "Epoch:48 > Train Loss: 4.7994, Train Acc: 0.9377\n",
      "Epoch:48 > Valid Loss: 2.3633, top(1): 0.7955,top(2): 0.8929, top(5): 0.9473, time: 85s \n",
      "========== Epoch: 49 training ==========\n",
      "Epoch:49 > Train Loss: 4.7991, Train Acc: 0.9381\n",
      "Epoch:49 > Valid Loss: 2.3648, top(1): 0.7957,top(2): 0.8927, top(5): 0.9469, time: 86s \n",
      "========== fold: 8 result ==========\n",
      "Score: 0.8028\n",
      "========== fold: 9 training ==========\n",
      "========== Epoch: 0 training ==========\n",
      "Epoch:0 > Train Loss: 5.3781, Train Acc: 0.1986\n",
      "Epoch:0 > Valid Loss: 3.8391, top(1): 0.4117,top(2): 0.5560, top(5): 0.7212, time: 87s \n",
      "Epoch 0 - Save Best Score: 0.4117 Model\n",
      "========== Epoch: 1 training ==========\n",
      "Epoch:1 > Train Loss: 5.1332, Train Acc: 0.5374\n",
      "Epoch:1 > Valid Loss: 3.0526, top(1): 0.5502,top(2): 0.7013, top(5): 0.8285, time: 87s \n",
      "Epoch 1 - Save Best Score: 0.5502 Model\n",
      "========== Epoch: 2 training ==========\n",
      "Epoch:2 > Train Loss: 5.0364, Train Acc: 0.6461\n",
      "Epoch:2 > Valid Loss: 2.9630, top(1): 0.6014,top(2): 0.7375, top(5): 0.8494, time: 87s \n",
      "Epoch 2 - Save Best Score: 0.6014 Model\n",
      "========== Epoch: 3 training ==========\n",
      "Epoch:3 > Train Loss: 4.9894, Train Acc: 0.7008\n",
      "Epoch:3 > Valid Loss: 2.7086, top(1): 0.6487,top(2): 0.7738, top(5): 0.8802, time: 87s \n",
      "Epoch 3 - Save Best Score: 0.6487 Model\n",
      "========== Epoch: 4 training ==========\n",
      "Epoch:4 > Train Loss: 4.9611, Train Acc: 0.7344\n",
      "Epoch:4 > Valid Loss: 2.6177, top(1): 0.6568,top(2): 0.7836, top(5): 0.8861, time: 86s \n",
      "Epoch 4 - Save Best Score: 0.6571 Model\n",
      "========== Epoch: 5 training ==========\n",
      "Epoch:5 > Train Loss: 4.9403, Train Acc: 0.7608\n",
      "Epoch:5 > Valid Loss: 2.7638, top(1): 0.6646,top(2): 0.7869, top(5): 0.8842, time: 87s \n",
      "Epoch 5 - Save Best Score: 0.6648 Model\n",
      "========== Epoch: 6 training ==========\n",
      "Epoch:6 > Train Loss: 4.9247, Train Acc: 0.7794\n",
      "Epoch:6 > Valid Loss: 2.8158, top(1): 0.6419,top(2): 0.7691, top(5): 0.8718, time: 86s \n",
      "========== Epoch: 7 training ==========\n",
      "Epoch:7 > Train Loss: 4.9102, Train Acc: 0.7982\n",
      "Epoch:7 > Valid Loss: 2.7668, top(1): 0.6461,top(2): 0.7726, top(5): 0.8767, time: 86s \n",
      "========== Epoch: 8 training ==========\n",
      "Epoch:8 > Train Loss: 4.8984, Train Acc: 0.8121\n",
      "Epoch:8 > Valid Loss: 2.6415, top(1): 0.6751,top(2): 0.8007, top(5): 0.8849, time: 87s \n",
      "Epoch 8 - Save Best Score: 0.6751 Model\n",
      "========== Epoch: 9 training ==========\n",
      "Epoch:9 > Train Loss: 4.8892, Train Acc: 0.8238\n",
      "Epoch:9 > Valid Loss: 2.7074, top(1): 0.6629,top(2): 0.7986, top(5): 0.8908, time: 86s \n",
      "========== Epoch: 10 training ==========\n",
      "Epoch:10 > Train Loss: 4.8780, Train Acc: 0.8382\n",
      "Epoch:10 > Valid Loss: 2.6783, top(1): 0.6830,top(2): 0.8023, top(5): 0.8884, time: 87s \n",
      "Epoch 10 - Save Best Score: 0.6830 Model\n",
      "========== Epoch: 11 training ==========\n",
      "Epoch:11 > Train Loss: 4.8697, Train Acc: 0.8480\n",
      "Epoch:11 > Valid Loss: 2.6474, top(1): 0.6844,top(2): 0.8082, top(5): 0.8891, time: 87s \n",
      "Epoch 11 - Save Best Score: 0.6844 Model\n",
      "========== Epoch: 12 training ==========\n",
      "Epoch:12 > Train Loss: 4.8617, Train Acc: 0.8591\n",
      "Epoch:12 > Valid Loss: 2.7101, top(1): 0.6739,top(2): 0.8014, top(5): 0.8812, time: 86s \n",
      "========== Epoch: 13 training ==========\n",
      "Epoch:13 > Train Loss: 4.8550, Train Acc: 0.8672\n",
      "Epoch:13 > Valid Loss: 2.6348, top(1): 0.6877,top(2): 0.8082, top(5): 0.8889, time: 86s \n",
      "Epoch 13 - Save Best Score: 0.6877 Model\n",
      "========== Epoch: 14 training ==========\n",
      "Epoch:14 > Train Loss: 4.8477, Train Acc: 0.8776\n",
      "Epoch:14 > Valid Loss: 2.6989, top(1): 0.6880,top(2): 0.8051, top(5): 0.8919, time: 86s \n",
      "Epoch 14 - Save Best Score: 0.6882 Model\n",
      "========== Epoch: 15 training ==========\n",
      "Epoch:15 > Train Loss: 5.0121, Train Acc: 0.7918\n",
      "Epoch:15 > Valid Loss: 2.7691, top(1): 0.6896,top(2): 0.8108, top(5): 0.8968, time: 87s \n",
      "Epoch 15 - Save Best Score: 0.6894 Model\n",
      "========== Epoch: 16 training ==========\n",
      "Epoch:16 > Train Loss: 4.9633, Train Acc: 0.8155\n",
      "Epoch:16 > Valid Loss: 2.7890, top(1): 0.6964,top(2): 0.8185, top(5): 0.8985, time: 87s \n",
      "Epoch 16 - Save Best Score: 0.6966 Model\n",
      "========== Epoch: 17 training ==========\n",
      "Epoch:17 > Train Loss: 4.9477, Train Acc: 0.8244\n",
      "Epoch:17 > Valid Loss: 2.7743, top(1): 0.6868,top(2): 0.8026, top(5): 0.8961, time: 87s \n",
      "========== Epoch: 18 training ==========\n",
      "Epoch:18 > Train Loss: 4.9390, Train Acc: 0.8307\n",
      "Epoch:18 > Valid Loss: 2.7677, top(1): 0.6936,top(2): 0.8159, top(5): 0.8959, time: 87s \n",
      "========== Epoch: 19 training ==========\n",
      "Epoch:19 > Train Loss: 4.9305, Train Acc: 0.8380\n",
      "Epoch:19 > Valid Loss: 2.7710, top(1): 0.6957,top(2): 0.8199, top(5): 0.8996, time: 87s \n",
      "========== Epoch: 20 training ==========\n",
      "Epoch:20 > Train Loss: 4.9233, Train Acc: 0.8443\n",
      "Epoch:20 > Valid Loss: 2.8622, top(1): 0.6915,top(2): 0.8129, top(5): 0.8940, time: 86s \n",
      "========== Epoch: 21 training ==========\n",
      "Epoch:21 > Train Loss: 4.9187, Train Acc: 0.8482\n",
      "Epoch:21 > Valid Loss: 2.7313, top(1): 0.7018,top(2): 0.8218, top(5): 0.9011, time: 86s \n",
      "Epoch 21 - Save Best Score: 0.7018 Model\n",
      "========== Epoch: 22 training ==========\n",
      "Epoch:22 > Train Loss: 4.9134, Train Acc: 0.8549\n",
      "Epoch:22 > Valid Loss: 2.8111, top(1): 0.6919,top(2): 0.8136, top(5): 0.8975, time: 86s \n",
      "========== Epoch: 23 training ==========\n",
      "Epoch:23 > Train Loss: 4.9082, Train Acc: 0.8601\n",
      "Epoch:23 > Valid Loss: 2.8434, top(1): 0.6884,top(2): 0.8115, top(5): 0.8971, time: 86s \n",
      "========== Epoch: 24 training ==========\n",
      "Epoch:24 > Train Loss: 4.9030, Train Acc: 0.8664\n",
      "Epoch:24 > Valid Loss: 2.8332, top(1): 0.6980,top(2): 0.8204, top(5): 0.9011, time: 87s \n",
      "========== Epoch: 25 training ==========\n",
      "Epoch:25 > Train Loss: 4.8982, Train Acc: 0.8706\n",
      "Epoch:25 > Valid Loss: 2.7898, top(1): 0.7004,top(2): 0.8260, top(5): 0.9036, time: 87s \n",
      "========== Epoch: 26 training ==========\n",
      "Epoch:26 > Train Loss: 4.8945, Train Acc: 0.8741\n",
      "Epoch:26 > Valid Loss: 2.8054, top(1): 0.6903,top(2): 0.8185, top(5): 0.9015, time: 87s \n",
      "========== Epoch: 27 training ==========\n",
      "Epoch:27 > Train Loss: 4.8907, Train Acc: 0.8795\n",
      "Epoch:27 > Valid Loss: 2.8056, top(1): 0.7006,top(2): 0.8194, top(5): 0.8996, time: 86s \n",
      "========== Epoch: 28 training ==========\n",
      "Epoch:28 > Train Loss: 4.8859, Train Acc: 0.8842\n",
      "Epoch:28 > Valid Loss: 2.8215, top(1): 0.7060,top(2): 0.8250, top(5): 0.9018, time: 86s \n",
      "Epoch 28 - Save Best Score: 0.7060 Model\n",
      "========== Epoch: 29 training ==========\n",
      "Epoch:29 > Train Loss: 4.8829, Train Acc: 0.8881\n",
      "Epoch:29 > Valid Loss: 2.8332, top(1): 0.7043,top(2): 0.8189, top(5): 0.9025, time: 86s \n",
      "========== Epoch: 30 training ==========\n",
      "Epoch:30 > Train Loss: 4.8792, Train Acc: 0.8914\n",
      "Epoch:30 > Valid Loss: 2.7677, top(1): 0.7001,top(2): 0.8166, top(5): 0.9041, time: 86s \n",
      "========== Epoch: 31 training ==========\n",
      "Epoch:31 > Train Loss: 4.8752, Train Acc: 0.8948\n",
      "Epoch:31 > Valid Loss: 2.8011, top(1): 0.7004,top(2): 0.8232, top(5): 0.9029, time: 87s \n",
      "========== Epoch: 32 training ==========\n",
      "Epoch:32 > Train Loss: 4.8727, Train Acc: 0.8993\n",
      "Epoch:32 > Valid Loss: 2.8224, top(1): 0.7062,top(2): 0.8206, top(5): 0.9050, time: 87s \n",
      "Epoch 32 - Save Best Score: 0.7062 Model\n",
      "========== Epoch: 33 training ==========\n",
      "Epoch:33 > Train Loss: 4.8690, Train Acc: 0.9035\n",
      "Epoch:33 > Valid Loss: 2.7735, top(1): 0.7048,top(2): 0.8267, top(5): 0.9050, time: 87s \n",
      "========== Epoch: 34 training ==========\n",
      "Epoch:34 > Train Loss: 4.8661, Train Acc: 0.9071\n",
      "Epoch:34 > Valid Loss: 2.7899, top(1): 0.7111,top(2): 0.8323, top(5): 0.9048, time: 87s \n",
      "Epoch 34 - Save Best Score: 0.7111 Model\n",
      "========== Epoch: 35 training ==========\n",
      "Epoch:35 > Train Loss: 4.8635, Train Acc: 0.9090\n",
      "Epoch:35 > Valid Loss: 2.8076, top(1): 0.7074,top(2): 0.8302, top(5): 0.9025, time: 87s \n",
      "========== Epoch: 36 training ==========\n",
      "Epoch:36 > Train Loss: 4.8174, Train Acc: 0.9251\n",
      "Epoch:36 > Valid Loss: 2.7253, top(1): 0.7113,top(2): 0.8257, top(5): 0.9006, time: 87s \n",
      "Epoch 36 - Save Best Score: 0.7113 Model\n",
      "========== Epoch: 37 training ==========\n",
      "Epoch:37 > Train Loss: 4.8135, Train Acc: 0.9300\n",
      "Epoch:37 > Valid Loss: 2.7261, top(1): 0.7106,top(2): 0.8295, top(5): 0.9025, time: 86s \n",
      "========== Epoch: 38 training ==========\n",
      "Epoch:38 > Train Loss: 4.8111, Train Acc: 0.9328\n",
      "Epoch:38 > Valid Loss: 2.7383, top(1): 0.7055,top(2): 0.8218, top(5): 0.9001, time: 86s \n",
      "========== Epoch: 39 training ==========\n",
      "Epoch:39 > Train Loss: 4.8090, Train Acc: 0.9347\n",
      "Epoch:39 > Valid Loss: 2.7307, top(1): 0.7111,top(2): 0.8274, top(5): 0.9006, time: 86s \n",
      "========== Epoch: 40 training ==========\n",
      "Epoch:40 > Train Loss: 4.8071, Train Acc: 0.9367\n",
      "Epoch:40 > Valid Loss: 2.7472, top(1): 0.7092,top(2): 0.8267, top(5): 0.9020, time: 86s \n",
      "========== Epoch: 41 training ==========\n",
      "Epoch:41 > Train Loss: 4.8055, Train Acc: 0.9389\n",
      "Epoch:41 > Valid Loss: 2.7517, top(1): 0.7067,top(2): 0.8276, top(5): 0.9001, time: 87s \n",
      "========== Epoch: 42 training ==========\n",
      "Epoch:42 > Train Loss: 4.8039, Train Acc: 0.9408\n",
      "Epoch:42 > Valid Loss: 2.7525, top(1): 0.7083,top(2): 0.8281, top(5): 0.9013, time: 87s \n",
      "========== Epoch: 43 training ==========\n",
      "Epoch:43 > Train Loss: 4.8030, Train Acc: 0.9423\n",
      "Epoch:43 > Valid Loss: 2.7542, top(1): 0.7104,top(2): 0.8257, top(5): 0.9015, time: 87s \n",
      "========== Epoch: 44 training ==========\n",
      "Epoch:44 > Train Loss: 4.8019, Train Acc: 0.9436\n",
      "Epoch:44 > Valid Loss: 2.7488, top(1): 0.7102,top(2): 0.8267, top(5): 0.9011, time: 86s \n",
      "========== Epoch: 45 training ==========\n",
      "Epoch:45 > Train Loss: 4.8011, Train Acc: 0.9434\n",
      "Epoch:45 > Valid Loss: 2.7612, top(1): 0.7102,top(2): 0.8255, top(5): 0.8994, time: 87s \n",
      "========== Epoch: 46 training ==========\n",
      "Epoch:46 > Train Loss: 4.8005, Train Acc: 0.9438\n",
      "Epoch:46 > Valid Loss: 2.7538, top(1): 0.7104,top(2): 0.8274, top(5): 0.9029, time: 87s \n",
      "========== Epoch: 47 training ==========\n",
      "Epoch:47 > Train Loss: 4.8001, Train Acc: 0.9449\n",
      "Epoch:47 > Valid Loss: 2.7520, top(1): 0.7104,top(2): 0.8285, top(5): 0.9039, time: 85s \n",
      "========== Epoch: 48 training ==========\n",
      "Epoch:48 > Train Loss: 4.7999, Train Acc: 0.9454\n",
      "Epoch:48 > Valid Loss: 2.7563, top(1): 0.7118,top(2): 0.8271, top(5): 0.9022, time: 86s \n",
      "Epoch 48 - Save Best Score: 0.7120 Model\n",
      "========== Epoch: 49 training ==========\n",
      "Epoch:49 > Train Loss: 4.8000, Train Acc: 0.9454\n",
      "Epoch:49 > Valid Loss: 2.7577, top(1): 0.7113,top(2): 0.8276, top(5): 0.9020, time: 86s \n",
      "========== fold: 9 result ==========\n",
      "Score: 0.7120\n",
      "========== fold: 10 training ==========\n",
      "========== Epoch: 0 training ==========\n",
      "Epoch:0 > Train Loss: 5.3806, Train Acc: 0.1921\n",
      "Epoch:0 > Valid Loss: 3.7306, top(1): 0.4216,top(2): 0.5709, top(5): 0.7220, time: 85s \n",
      "Epoch 0 - Save Best Score: 0.4219 Model\n",
      "========== Epoch: 1 training ==========\n",
      "Epoch:1 > Train Loss: 5.1442, Train Acc: 0.5226\n",
      "Epoch:1 > Valid Loss: 3.0075, top(1): 0.6042,top(2): 0.7285, top(5): 0.8407, time: 86s \n",
      "Epoch 1 - Save Best Score: 0.6042 Model\n",
      "========== Epoch: 2 training ==========\n",
      "Epoch:2 > Train Loss: 5.0466, Train Acc: 0.6336\n",
      "Epoch:2 > Valid Loss: 2.7291, top(1): 0.6564,top(2): 0.7704, top(5): 0.8747, time: 84s \n",
      "Epoch 2 - Save Best Score: 0.6564 Model\n",
      "========== Epoch: 3 training ==========\n",
      "Epoch:3 > Train Loss: 4.9995, Train Acc: 0.6860\n",
      "Epoch:3 > Valid Loss: 2.5930, top(1): 0.6919,top(2): 0.7950, top(5): 0.8854, time: 85s \n",
      "Epoch 3 - Save Best Score: 0.6921 Model\n",
      "========== Epoch: 4 training ==========\n",
      "Epoch:4 > Train Loss: 4.9691, Train Acc: 0.7204\n",
      "Epoch:4 > Valid Loss: 2.5225, top(1): 0.7210,top(2): 0.8200, top(5): 0.8901, time: 86s \n",
      "Epoch 4 - Save Best Score: 0.7208 Model\n",
      "========== Epoch: 5 training ==========\n",
      "Epoch:5 > Train Loss: 4.9462, Train Acc: 0.7495\n",
      "Epoch:5 > Valid Loss: 2.5653, top(1): 0.7220,top(2): 0.8279, top(5): 0.8999, time: 85s \n",
      "Epoch 5 - Save Best Score: 0.7225 Model\n",
      "========== Epoch: 6 training ==========\n",
      "Epoch:6 > Train Loss: 4.9294, Train Acc: 0.7689\n",
      "Epoch:6 > Valid Loss: 2.3758, top(1): 0.7426,top(2): 0.8399, top(5): 0.9074, time: 85s \n",
      "Epoch 6 - Save Best Score: 0.7428 Model\n",
      "========== Epoch: 7 training ==========\n",
      "Epoch:7 > Train Loss: 4.9155, Train Acc: 0.7843\n",
      "Epoch:7 > Valid Loss: 2.4458, top(1): 0.7415,top(2): 0.8390, top(5): 0.9070, time: 85s \n",
      "========== Epoch: 8 training ==========\n",
      "Epoch:8 > Train Loss: 4.9011, Train Acc: 0.8024\n",
      "Epoch:8 > Valid Loss: 2.4497, top(1): 0.7443,top(2): 0.8448, top(5): 0.9145, time: 87s \n",
      "Epoch 8 - Save Best Score: 0.7443 Model\n",
      "========== Epoch: 9 training ==========\n",
      "Epoch:9 > Train Loss: 4.8915, Train Acc: 0.8157\n",
      "Epoch:9 > Valid Loss: 2.3175, top(1): 0.7695,top(2): 0.8597, top(5): 0.9134, time: 86s \n",
      "Epoch 9 - Save Best Score: 0.7693 Model\n",
      "========== Epoch: 10 training ==========\n",
      "Epoch:10 > Train Loss: 4.8821, Train Acc: 0.8268\n",
      "Epoch:10 > Valid Loss: 2.3772, top(1): 0.7537,top(2): 0.8441, top(5): 0.9087, time: 85s \n",
      "========== Epoch: 11 training ==========\n",
      "Epoch:11 > Train Loss: 4.8728, Train Acc: 0.8386\n",
      "Epoch:11 > Valid Loss: 2.4162, top(1): 0.7654,top(2): 0.8533, top(5): 0.9145, time: 86s \n",
      "========== Epoch: 12 training ==========\n",
      "Epoch:12 > Train Loss: 4.8645, Train Acc: 0.8482\n",
      "Epoch:12 > Valid Loss: 2.3064, top(1): 0.7654,top(2): 0.8574, top(5): 0.9170, time: 86s \n",
      "========== Epoch: 13 training ==========\n",
      "Epoch:13 > Train Loss: 4.8583, Train Acc: 0.8572\n",
      "Epoch:13 > Valid Loss: 2.2944, top(1): 0.7787,top(2): 0.8561, top(5): 0.9175, time: 85s \n",
      "Epoch 13 - Save Best Score: 0.7787 Model\n",
      "========== Epoch: 14 training ==========\n",
      "Epoch:14 > Train Loss: 4.8503, Train Acc: 0.8671\n",
      "Epoch:14 > Valid Loss: 2.3308, top(1): 0.7687,top(2): 0.8580, top(5): 0.9181, time: 86s \n",
      "========== Epoch: 15 training ==========\n",
      "Epoch:15 > Train Loss: 5.0132, Train Acc: 0.7862\n",
      "Epoch:15 > Valid Loss: 2.5500, top(1): 0.7642,top(2): 0.8544, top(5): 0.9194, time: 86s \n",
      "========== Epoch: 16 training ==========\n",
      "Epoch:16 > Train Loss: 4.9623, Train Acc: 0.8095\n",
      "Epoch:16 > Valid Loss: 2.5058, top(1): 0.7729,top(2): 0.8606, top(5): 0.9200, time: 86s \n",
      "========== Epoch: 17 training ==========\n",
      "Epoch:17 > Train Loss: 4.9472, Train Acc: 0.8184\n",
      "Epoch:17 > Valid Loss: 2.5109, top(1): 0.7710,top(2): 0.8625, top(5): 0.9175, time: 85s \n",
      "========== Epoch: 18 training ==========\n",
      "Epoch:18 > Train Loss: 4.9374, Train Acc: 0.8268\n",
      "Epoch:18 > Valid Loss: 2.5058, top(1): 0.7661,top(2): 0.8563, top(5): 0.9194, time: 85s \n",
      "========== Epoch: 19 training ==========\n",
      "Epoch:19 > Train Loss: 4.9301, Train Acc: 0.8316\n",
      "Epoch:19 > Valid Loss: 2.5807, top(1): 0.7672,top(2): 0.8589, top(5): 0.9196, time: 86s \n",
      "========== Epoch: 20 training ==========\n",
      "Epoch:20 > Train Loss: 4.9224, Train Acc: 0.8385\n",
      "Epoch:20 > Valid Loss: 2.5449, top(1): 0.7744,top(2): 0.8608, top(5): 0.9194, time: 86s \n",
      "========== Epoch: 21 training ==========\n",
      "Epoch:21 > Train Loss: 4.9165, Train Acc: 0.8449\n",
      "Epoch:21 > Valid Loss: 2.5167, top(1): 0.7770,top(2): 0.8659, top(5): 0.9192, time: 86s \n",
      "========== Epoch: 22 training ==========\n",
      "Epoch:22 > Train Loss: 4.9108, Train Acc: 0.8509\n",
      "Epoch:22 > Valid Loss: 2.5396, top(1): 0.7680,top(2): 0.8591, top(5): 0.9183, time: 85s \n",
      "========== Epoch: 23 training ==========\n",
      "Epoch:23 > Train Loss: 4.9059, Train Acc: 0.8558\n",
      "Epoch:23 > Valid Loss: 2.4926, top(1): 0.7851,top(2): 0.8689, top(5): 0.9220, time: 87s \n",
      "Epoch 23 - Save Best Score: 0.7851 Model\n",
      "========== Epoch: 24 training ==========\n",
      "Epoch:24 > Train Loss: 4.9012, Train Acc: 0.8614\n",
      "Epoch:24 > Valid Loss: 2.4852, top(1): 0.7830,top(2): 0.8743, top(5): 0.9226, time: 86s \n",
      "========== Epoch: 25 training ==========\n",
      "Epoch:25 > Train Loss: 4.8957, Train Acc: 0.8666\n",
      "Epoch:25 > Valid Loss: 2.4893, top(1): 0.7849,top(2): 0.8687, top(5): 0.9250, time: 85s \n",
      "========== Epoch: 26 training ==========\n",
      "Epoch:26 > Train Loss: 4.8922, Train Acc: 0.8703\n",
      "Epoch:26 > Valid Loss: 2.5084, top(1): 0.7834,top(2): 0.8687, top(5): 0.9230, time: 86s \n",
      "========== Epoch: 27 training ==========\n",
      "Epoch:27 > Train Loss: 4.8882, Train Acc: 0.8744\n",
      "Epoch:27 > Valid Loss: 2.4730, top(1): 0.7909,top(2): 0.8753, top(5): 0.9254, time: 85s \n",
      "Epoch 27 - Save Best Score: 0.7909 Model\n",
      "========== Epoch: 28 training ==========\n",
      "Epoch:28 > Train Loss: 4.8849, Train Acc: 0.8783\n",
      "Epoch:28 > Valid Loss: 2.4990, top(1): 0.7802,top(2): 0.8666, top(5): 0.9226, time: 85s \n",
      "========== Epoch: 29 training ==========\n",
      "Epoch:29 > Train Loss: 4.8805, Train Acc: 0.8825\n",
      "Epoch:29 > Valid Loss: 2.4850, top(1): 0.7826,top(2): 0.8677, top(5): 0.9241, time: 85s \n",
      "========== Epoch: 30 training ==========\n",
      "Epoch:30 > Train Loss: 4.8770, Train Acc: 0.8869\n",
      "Epoch:30 > Valid Loss: 2.4710, top(1): 0.7843,top(2): 0.8698, top(5): 0.9252, time: 85s \n",
      "========== Epoch: 31 training ==========\n",
      "Epoch:31 > Train Loss: 4.8734, Train Acc: 0.8913\n",
      "Epoch:31 > Valid Loss: 2.4807, top(1): 0.7898,top(2): 0.8777, top(5): 0.9269, time: 86s \n",
      "========== Epoch: 32 training ==========\n",
      "Epoch:32 > Train Loss: 4.8697, Train Acc: 0.8955\n",
      "Epoch:32 > Valid Loss: 2.4673, top(1): 0.7866,top(2): 0.8730, top(5): 0.9243, time: 86s \n",
      "========== Epoch: 33 training ==========\n",
      "Epoch:33 > Train Loss: 4.8663, Train Acc: 0.8983\n",
      "Epoch:33 > Valid Loss: 2.5005, top(1): 0.7945,top(2): 0.8760, top(5): 0.9252, time: 86s \n",
      "Epoch 33 - Save Best Score: 0.7947 Model\n",
      "========== Epoch: 34 training ==========\n",
      "Epoch:34 > Train Loss: 4.8640, Train Acc: 0.9022\n",
      "Epoch:34 > Valid Loss: 2.4720, top(1): 0.7941,top(2): 0.8743, top(5): 0.9258, time: 86s \n",
      "========== Epoch: 35 training ==========\n",
      "Epoch:35 > Train Loss: 4.8605, Train Acc: 0.9052\n",
      "Epoch:35 > Valid Loss: 2.4930, top(1): 0.7907,top(2): 0.8719, top(5): 0.9224, time: 85s \n",
      "========== Epoch: 36 training ==========\n",
      "Epoch:36 > Train Loss: 4.8159, Train Acc: 0.9202\n",
      "Epoch:36 > Valid Loss: 2.3985, top(1): 0.7965,top(2): 0.8820, top(5): 0.9264, time: 86s \n",
      "Epoch 36 - Save Best Score: 0.7967 Model\n",
      "========== Epoch: 37 training ==========\n",
      "Epoch:37 > Train Loss: 4.8120, Train Acc: 0.9233\n",
      "Epoch:37 > Valid Loss: 2.4411, top(1): 0.7958,top(2): 0.8771, top(5): 0.9256, time: 86s \n",
      "========== Epoch: 38 training ==========\n",
      "Epoch:38 > Train Loss: 4.8096, Train Acc: 0.9269\n",
      "Epoch:38 > Valid Loss: 2.4259, top(1): 0.7956,top(2): 0.8773, top(5): 0.9279, time: 86s \n",
      "========== Epoch: 39 training ==========\n",
      "Epoch:39 > Train Loss: 4.8073, Train Acc: 0.9292\n",
      "Epoch:39 > Valid Loss: 2.3946, top(1): 0.7962,top(2): 0.8775, top(5): 0.9247, time: 86s \n",
      "========== Epoch: 40 training ==========\n",
      "Epoch:40 > Train Loss: 4.8055, Train Acc: 0.9316\n",
      "Epoch:40 > Valid Loss: 2.4350, top(1): 0.7950,top(2): 0.8766, top(5): 0.9264, time: 85s \n",
      "========== Epoch: 41 training ==========\n",
      "Epoch:41 > Train Loss: 4.8038, Train Acc: 0.9325\n",
      "Epoch:41 > Valid Loss: 2.4110, top(1): 0.7956,top(2): 0.8768, top(5): 0.9271, time: 85s \n",
      "========== Epoch: 42 training ==========\n",
      "Epoch:42 > Train Loss: 4.8023, Train Acc: 0.9346\n",
      "Epoch:42 > Valid Loss: 2.4233, top(1): 0.7943,top(2): 0.8771, top(5): 0.9239, time: 85s \n",
      "========== Epoch: 43 training ==========\n",
      "Epoch:43 > Train Loss: 4.8010, Train Acc: 0.9359\n",
      "Epoch:43 > Valid Loss: 2.4231, top(1): 0.7937,top(2): 0.8775, top(5): 0.9264, time: 86s \n",
      "========== Epoch: 44 training ==========\n",
      "Epoch:44 > Train Loss: 4.8002, Train Acc: 0.9368\n",
      "Epoch:44 > Valid Loss: 2.4291, top(1): 0.7932,top(2): 0.8739, top(5): 0.9250, time: 86s \n",
      "========== Epoch: 45 training ==========\n",
      "Epoch:45 > Train Loss: 4.7991, Train Acc: 0.9374\n",
      "Epoch:45 > Valid Loss: 2.4313, top(1): 0.7926,top(2): 0.8766, top(5): 0.9247, time: 86s \n",
      "========== Epoch: 46 training ==========\n",
      "Epoch:46 > Train Loss: 4.7988, Train Acc: 0.9384\n",
      "Epoch:46 > Valid Loss: 2.4308, top(1): 0.7924,top(2): 0.8760, top(5): 0.9254, time: 86s \n",
      "========== Epoch: 47 training ==========\n",
      "Epoch:47 > Train Loss: 4.7984, Train Acc: 0.9390\n",
      "Epoch:47 > Valid Loss: 2.4305, top(1): 0.7930,top(2): 0.8771, top(5): 0.9262, time: 85s \n",
      "========== Epoch: 48 training ==========\n",
      "Epoch:48 > Train Loss: 4.7977, Train Acc: 0.9394\n",
      "Epoch:48 > Valid Loss: 2.4305, top(1): 0.7926,top(2): 0.8762, top(5): 0.9262, time: 85s \n",
      "========== Epoch: 49 training ==========\n",
      "Epoch:49 > Train Loss: 4.7975, Train Acc: 0.9393\n",
      "Epoch:49 > Valid Loss: 2.4347, top(1): 0.7926,top(2): 0.8766, top(5): 0.9267, time: 87s \n",
      "========== fold: 10 result ==========\n",
      "Score: 0.7967\n",
      "========== fold: 18 training ==========\n",
      "========== Epoch: 0 training ==========\n",
      "Epoch:0 > Train Loss: 5.3826, Train Acc: 0.1900\n",
      "Epoch:0 > Valid Loss: 3.6268, top(1): 0.4468,top(2): 0.6075, top(5): 0.7773, time: 85s \n",
      "Epoch 0 - Save Best Score: 0.4470 Model\n",
      "========== Epoch: 1 training ==========\n",
      "Epoch:1 > Train Loss: 5.1437, Train Acc: 0.5230\n",
      "Epoch:1 > Valid Loss: 2.8502, top(1): 0.6322,top(2): 0.7653, top(5): 0.8798, time: 85s \n",
      "Epoch 1 - Save Best Score: 0.6324 Model\n",
      "========== Epoch: 2 training ==========\n",
      "Epoch:2 > Train Loss: 5.0450, Train Acc: 0.6345\n",
      "Epoch:2 > Valid Loss: 2.6053, top(1): 0.6788,top(2): 0.8179, top(5): 0.9129, time: 85s \n",
      "Epoch 2 - Save Best Score: 0.6786 Model\n",
      "========== Epoch: 3 training ==========\n",
      "Epoch:3 > Train Loss: 4.9969, Train Acc: 0.6886\n",
      "Epoch:3 > Valid Loss: 2.4958, top(1): 0.7139,top(2): 0.8322, top(5): 0.9264, time: 85s \n",
      "Epoch 3 - Save Best Score: 0.7139 Model\n",
      "========== Epoch: 4 training ==========\n",
      "Epoch:4 > Train Loss: 4.9657, Train Acc: 0.7257\n",
      "Epoch:4 > Valid Loss: 2.5004, top(1): 0.7183,top(2): 0.8399, top(5): 0.9274, time: 84s \n",
      "Epoch 4 - Save Best Score: 0.7183 Model\n",
      "========== Epoch: 5 training ==========\n",
      "Epoch:5 > Train Loss: 4.9458, Train Acc: 0.7493\n",
      "Epoch:5 > Valid Loss: 2.4949, top(1): 0.7281,top(2): 0.8495, top(5): 0.9295, time: 84s \n",
      "Epoch 5 - Save Best Score: 0.7281 Model\n",
      "========== Epoch: 6 training ==========\n",
      "Epoch:6 > Train Loss: 4.9270, Train Acc: 0.7709\n",
      "Epoch:6 > Valid Loss: 2.3790, top(1): 0.7424,top(2): 0.8541, top(5): 0.9304, time: 85s \n",
      "Epoch 6 - Save Best Score: 0.7424 Model\n",
      "========== Epoch: 7 training ==========\n",
      "Epoch:7 > Train Loss: 4.9134, Train Acc: 0.7905\n",
      "Epoch:7 > Valid Loss: 2.3282, top(1): 0.7597,top(2): 0.8657, top(5): 0.9393, time: 85s \n",
      "Epoch 7 - Save Best Score: 0.7599 Model\n",
      "========== Epoch: 8 training ==========\n",
      "Epoch:8 > Train Loss: 4.9014, Train Acc: 0.8034\n",
      "Epoch:8 > Valid Loss: 2.4155, top(1): 0.7528,top(2): 0.8563, top(5): 0.9403, time: 85s \n",
      "========== Epoch: 9 training ==========\n",
      "Epoch:9 > Train Loss: 4.8903, Train Acc: 0.8168\n",
      "Epoch:9 > Valid Loss: 2.2694, top(1): 0.7520,top(2): 0.8721, top(5): 0.9395, time: 84s \n",
      "========== Epoch: 10 training ==========\n",
      "Epoch:10 > Train Loss: 4.8815, Train Acc: 0.8279\n",
      "Epoch:10 > Valid Loss: 2.3916, top(1): 0.7555,top(2): 0.8682, top(5): 0.9405, time: 84s \n",
      "========== Epoch: 11 training ==========\n",
      "Epoch:11 > Train Loss: 4.8732, Train Acc: 0.8397\n",
      "Epoch:11 > Valid Loss: 2.2792, top(1): 0.7786,top(2): 0.8844, top(5): 0.9445, time: 84s \n",
      "Epoch 11 - Save Best Score: 0.7786 Model\n",
      "========== Epoch: 12 training ==========\n",
      "Epoch:12 > Train Loss: 4.8654, Train Acc: 0.8497\n",
      "Epoch:12 > Valid Loss: 2.3192, top(1): 0.7657,top(2): 0.8688, top(5): 0.9410, time: 85s \n",
      "========== Epoch: 13 training ==========\n",
      "Epoch:13 > Train Loss: 4.8559, Train Acc: 0.8620\n",
      "Epoch:13 > Valid Loss: 2.3630, top(1): 0.7601,top(2): 0.8682, top(5): 0.9383, time: 84s \n",
      "========== Epoch: 14 training ==========\n",
      "Epoch:14 > Train Loss: 4.8495, Train Acc: 0.8702\n",
      "Epoch:14 > Valid Loss: 2.3976, top(1): 0.7547,top(2): 0.8630, top(5): 0.9399, time: 86s \n",
      "========== Epoch: 15 training ==========\n",
      "Epoch:15 > Train Loss: 5.0122, Train Acc: 0.7895\n",
      "Epoch:15 > Valid Loss: 2.4882, top(1): 0.7559,top(2): 0.8732, top(5): 0.9397, time: 84s \n",
      "========== Epoch: 16 training ==========\n",
      "Epoch:16 > Train Loss: 4.9637, Train Acc: 0.8094\n",
      "Epoch:16 > Valid Loss: 2.4534, top(1): 0.7701,top(2): 0.8771, top(5): 0.9439, time: 86s \n",
      "========== Epoch: 17 training ==========\n",
      "Epoch:17 > Train Loss: 4.9468, Train Acc: 0.8215\n",
      "Epoch:17 > Valid Loss: 2.4588, top(1): 0.7601,top(2): 0.8684, top(5): 0.9414, time: 85s \n",
      "========== Epoch: 18 training ==========\n",
      "Epoch:18 > Train Loss: 4.9363, Train Acc: 0.8292\n",
      "Epoch:18 > Valid Loss: 2.5397, top(1): 0.7493,top(2): 0.8653, top(5): 0.9405, time: 86s \n",
      "========== Epoch: 19 training ==========\n",
      "Epoch:19 > Train Loss: 4.9305, Train Acc: 0.8326\n",
      "Epoch:19 > Valid Loss: 2.4655, top(1): 0.7649,top(2): 0.8738, top(5): 0.9432, time: 86s \n",
      "========== Epoch: 20 training ==========\n",
      "Epoch:20 > Train Loss: 4.9227, Train Acc: 0.8416\n",
      "Epoch:20 > Valid Loss: 2.4885, top(1): 0.7563,top(2): 0.8699, top(5): 0.9447, time: 86s \n",
      "========== Epoch: 21 training ==========\n",
      "Epoch:21 > Train Loss: 4.9173, Train Acc: 0.8453\n",
      "Epoch:21 > Valid Loss: 2.5177, top(1): 0.7563,top(2): 0.8674, top(5): 0.9414, time: 85s \n",
      "========== Epoch: 22 training ==========\n",
      "Epoch:22 > Train Loss: 4.9116, Train Acc: 0.8510\n",
      "Epoch:22 > Valid Loss: 2.4703, top(1): 0.7667,top(2): 0.8748, top(5): 0.9439, time: 86s \n",
      "========== Epoch: 23 training ==========\n",
      "Epoch:23 > Train Loss: 4.9072, Train Acc: 0.8561\n",
      "Epoch:23 > Valid Loss: 2.4661, top(1): 0.7661,top(2): 0.8748, top(5): 0.9449, time: 86s \n",
      "========== Epoch: 24 training ==========\n",
      "Epoch:24 > Train Loss: 4.9017, Train Acc: 0.8611\n",
      "Epoch:24 > Valid Loss: 2.5431, top(1): 0.7572,top(2): 0.8701, top(5): 0.9418, time: 86s \n",
      "========== Epoch: 25 training ==========\n",
      "Epoch:25 > Train Loss: 4.8969, Train Acc: 0.8673\n",
      "Epoch:25 > Valid Loss: 2.4165, top(1): 0.7699,top(2): 0.8751, top(5): 0.9441, time: 86s \n",
      "========== Epoch: 26 training ==========\n",
      "Epoch:26 > Train Loss: 4.8932, Train Acc: 0.8704\n",
      "Epoch:26 > Valid Loss: 2.4607, top(1): 0.7730,top(2): 0.8775, top(5): 0.9447, time: 86s \n",
      "========== Epoch: 27 training ==========\n",
      "Epoch:27 > Train Loss: 4.8879, Train Acc: 0.8764\n",
      "Epoch:27 > Valid Loss: 2.4933, top(1): 0.7590,top(2): 0.8736, top(5): 0.9407, time: 86s \n",
      "========== Epoch: 28 training ==========\n",
      "Epoch:28 > Train Loss: 4.8850, Train Acc: 0.8795\n",
      "Epoch:28 > Valid Loss: 2.5103, top(1): 0.7630,top(2): 0.8705, top(5): 0.9385, time: 86s \n",
      "========== Epoch: 29 training ==========\n",
      "Epoch:29 > Train Loss: 4.8806, Train Acc: 0.8843\n",
      "Epoch:29 > Valid Loss: 2.5010, top(1): 0.7717,top(2): 0.8775, top(5): 0.9468, time: 87s \n",
      "========== Epoch: 30 training ==========\n",
      "Epoch:30 > Train Loss: 4.8768, Train Acc: 0.8879\n",
      "Epoch:30 > Valid Loss: 2.5059, top(1): 0.7732,top(2): 0.8775, top(5): 0.9443, time: 84s \n",
      "========== Epoch: 31 training ==========\n",
      "Epoch:31 > Train Loss: 4.8732, Train Acc: 0.8929\n",
      "Epoch:31 > Valid Loss: 2.5166, top(1): 0.7632,top(2): 0.8713, top(5): 0.9441, time: 85s \n",
      "========== Epoch: 32 training ==========\n",
      "Epoch:32 > Train Loss: 4.8699, Train Acc: 0.8967\n",
      "Epoch:32 > Valid Loss: 2.4775, top(1): 0.7699,top(2): 0.8753, top(5): 0.9457, time: 84s \n",
      "========== Epoch: 33 training ==========\n",
      "Epoch:33 > Train Loss: 4.8667, Train Acc: 0.9003\n",
      "Epoch:33 > Valid Loss: 2.4965, top(1): 0.7626,top(2): 0.8794, top(5): 0.9455, time: 85s \n",
      "========== Epoch: 34 training ==========\n",
      "Epoch:34 > Train Loss: 4.8643, Train Acc: 0.9034\n",
      "Epoch:34 > Valid Loss: 2.5188, top(1): 0.7690,top(2): 0.8782, top(5): 0.9420, time: 85s \n",
      "========== Epoch: 35 training ==========\n",
      "Epoch:35 > Train Loss: 4.8609, Train Acc: 0.9067\n",
      "Epoch:35 > Valid Loss: 2.5414, top(1): 0.7642,top(2): 0.8721, top(5): 0.9407, time: 84s \n",
      "========== Epoch: 36 training ==========\n",
      "Epoch:36 > Train Loss: 4.8160, Train Acc: 0.9211\n",
      "Epoch:36 > Valid Loss: 2.4412, top(1): 0.7748,top(2): 0.8763, top(5): 0.9441, time: 85s \n",
      "========== Epoch: 37 training ==========\n",
      "Epoch:37 > Train Loss: 4.8119, Train Acc: 0.9264\n",
      "Epoch:37 > Valid Loss: 2.4508, top(1): 0.7717,top(2): 0.8794, top(5): 0.9414, time: 85s \n",
      "========== Epoch: 38 training ==========\n",
      "Epoch:38 > Train Loss: 4.8099, Train Acc: 0.9280\n",
      "Epoch:38 > Valid Loss: 2.4568, top(1): 0.7703,top(2): 0.8703, top(5): 0.9403, time: 84s \n",
      "========== Epoch: 39 training ==========\n",
      "Epoch:39 > Train Loss: 4.8073, Train Acc: 0.9301\n",
      "Epoch:39 > Valid Loss: 2.4552, top(1): 0.7665,top(2): 0.8769, top(5): 0.9451, time: 86s \n",
      "========== Epoch: 40 training ==========\n",
      "Epoch:40 > Train Loss: 4.8055, Train Acc: 0.9325\n",
      "Epoch:40 > Valid Loss: 2.4620, top(1): 0.7696,top(2): 0.8780, top(5): 0.9420, time: 85s \n",
      "========== Epoch: 41 training ==========\n",
      "Epoch:41 > Train Loss: 4.8038, Train Acc: 0.9342\n",
      "Epoch:41 > Valid Loss: 2.4577, top(1): 0.7680,top(2): 0.8746, top(5): 0.9432, time: 85s \n",
      "========== Epoch: 42 training ==========\n",
      "Epoch:42 > Train Loss: 4.8025, Train Acc: 0.9356\n",
      "Epoch:42 > Valid Loss: 2.4644, top(1): 0.7672,top(2): 0.8759, top(5): 0.9414, time: 85s \n",
      "========== Epoch: 43 training ==========\n",
      "Epoch:43 > Train Loss: 4.8010, Train Acc: 0.9375\n",
      "Epoch:43 > Valid Loss: 2.4682, top(1): 0.7692,top(2): 0.8757, top(5): 0.9407, time: 85s \n",
      "========== Epoch: 44 training ==========\n",
      "Epoch:44 > Train Loss: 4.8000, Train Acc: 0.9386\n",
      "Epoch:44 > Valid Loss: 2.4605, top(1): 0.7719,top(2): 0.8751, top(5): 0.9441, time: 84s \n",
      "========== Epoch: 45 training ==========\n",
      "Epoch:45 > Train Loss: 4.7993, Train Acc: 0.9388\n",
      "Epoch:45 > Valid Loss: 2.4590, top(1): 0.7723,top(2): 0.8759, top(5): 0.9420, time: 85s \n",
      "========== Epoch: 46 training ==========\n",
      "Epoch:46 > Train Loss: 4.7989, Train Acc: 0.9397\n",
      "Epoch:46 > Valid Loss: 2.4577, top(1): 0.7699,top(2): 0.8744, top(5): 0.9418, time: 85s \n",
      "========== Epoch: 47 training ==========\n",
      "Epoch:47 > Train Loss: 4.7983, Train Acc: 0.9398\n",
      "Epoch:47 > Valid Loss: 2.4591, top(1): 0.7707,top(2): 0.8740, top(5): 0.9420, time: 85s \n",
      "========== Epoch: 48 training ==========\n",
      "Epoch:48 > Train Loss: 4.7979, Train Acc: 0.9409\n",
      "Epoch:48 > Valid Loss: 2.4611, top(1): 0.7701,top(2): 0.8748, top(5): 0.9416, time: 84s \n",
      "========== Epoch: 49 training ==========\n",
      "Epoch:49 > Train Loss: 4.7978, Train Acc: 0.9408\n",
      "Epoch:49 > Valid Loss: 2.4647, top(1): 0.7699,top(2): 0.8746, top(5): 0.9416, time: 85s \n",
      "========== fold: 18 result ==========\n",
      "Score: 0.7786\n",
      "========== CV ==========\n",
      "Score: 0.7677\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':   \n",
    "    \n",
    "    def get_result(oof_df):\n",
    "        labels = oof_df[true_cols].values\n",
    "        preds = oof_df[pred_cols].values\n",
    "        score = get_score(preds, labels)\n",
    "        LOGGER.info(f'Score: {score:<.4f}')\n",
    "    \n",
    "    if CFG.train:\n",
    "        oof_df = pd.DataFrame()\n",
    "        for fold in range(CFG.n_fold):\n",
    "            if fold in CFG.trn_fold:\n",
    "                _oof_df = train_loop(df_folds, fold)\n",
    "                oof_df = pd.concat([oof_df, _oof_df])\n",
    "                _oof_df.to_csv(OUTPUT_DIR+f'oof_df{fold}.csv',index=False)\n",
    "                LOGGER.info(f\"========== fold: {fold} result ==========\")\n",
    "                get_result(_oof_df)\n",
    "        oof_df = oof_df.reset_index(drop=True)\n",
    "        LOGGER.info(f\"========== CV ==========\")\n",
    "        get_result(oof_df)\n",
    "        oof_df.to_pickle(OUTPUT_DIR+'oof_df.pkl')\n",
    "        oof_df.to_csv(OUTPUT_DIR+'oof_df.csv',index=False)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "e92d4d9225dd42f7e435c803f9f15fd11797adb18db5d0cb83e8578d253831ff"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
